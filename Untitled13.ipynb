{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUHb2s2XzAJYa4dXKHP5GI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KattyDan/hw.friday1/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5kcBsG4YzHD",
        "outputId": "4c45a200-0b7a-4e0b-9c10-14bf77e47f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "–°–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–í–û–°–¢–ï–ô\n",
            "============================================================\n",
            "\n",
            " –°–æ–±. 50 –Ω–æ–≤–æ—Å—Ç–µ–π —Å Lenta.ru\n",
            "–°—Ç—Ä–∞–Ω–∏—Ü–∞ 1: —Å–æ–±—Ä–∞–Ω–æ 20 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
            "–°—Ç—Ä–∞–Ω–∏—Ü–∞ 2: —Å–æ–±—Ä–∞–Ω–æ 40 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
            "–°—Ç—Ä–∞–Ω–∏—Ü–∞ 3: —Å–æ–±—Ä–∞–Ω–æ 50 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
            " –°–æ–±—Ä–∞–Ω–æ 50 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä—ã –Ω–æ–≤–æ—Å—Ç–µ–π:\n",
            "1. –≠–∫—Å-–ø—Ä–µ–º—å–µ—Ä –£–∫—Ä–∞–∏–Ω—ã –æ–±–≤–∏–Ω–∏–ª –ö–∏–µ–≤ –≤ –∫—Ä–∞–∂–µ 100 –º–∏–ª–ª–∏...\n",
            "2. –ù–∞ –ó–∞–ø–∞–¥–µ –æ–±–ª–∏—á–∏–ª–∏ –∂–µ–ª–∞–Ω–∏–µ –ö–∞–ª–ª–∞—Å —Ä–∞–∑–¥—Ä–æ–±–∏—Ç—å –†–æ—Å—Å–∏...\n",
            "3. –ö–∞—Ç–∞–≤—à–∏–π—Å—è –Ω–∞ –∫—Ä—ã—à–µ –ø–æ–µ–∑–¥–∞ –º–µ—Ç—Ä–æ —Ä–æ—Å—Å–∏—è–Ω–∏–Ω –ø–æ–ø–∞–ª –Ω...\n",
            "\n",
            " –û—á–∏—â–∞—é —Ç–µ–∫—Å—Ç—ã –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
            "\n",
            " –°–æ–∑–¥–∞—é —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
            " –ù–∞–π–¥–µ–Ω–æ 100 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
            "\n",
            "–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤:\n",
            "   1. –≤               - 232 —Ä–∞–∑\n",
            "   2. –Ω–∞              - 157 —Ä–∞–∑\n",
            "   3. –∏               - 133 —Ä–∞–∑\n",
            "   4. —á—Ç–æ             - 116 —Ä–∞–∑\n",
            "   5. –ø–æ              -  88 —Ä–∞–∑\n",
            "   6. —Å               -  70 —Ä–∞–∑\n",
            "   7. –Ω–µ              -  51 —Ä–∞–∑\n",
            "   8. –æ–Ω              -  48 —Ä–∞–∑\n",
            "   9. —ç—Ç–æ–º            -  40 —Ä–∞–∑\n",
            "  10. –µ–≥–æ             -  35 —Ä–∞–∑\n",
            "\n",
            " –°–æ–∑–¥–∞—é Bag of Words–≠\n",
            " –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: 100 —Å–ª–æ–≤\n",
            " –†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã: 50 x 100\n",
            "\n",
            " –°–æ—Ö—Ä–∞–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.\n",
            " –ù–æ–≤–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ news.csv\n",
            " –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ word_counts.csv\n",
            " Bag of Words –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ bag_of_words.csv\n",
            "\n",
            " –ü—Ä–æ—Å—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
            " –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ normalized_news.csv\n",
            "\n",
            " –°–æ–∑–¥–∞—é –ø—Ä–æ—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (one-hot –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)\n",
            " One-hot —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ simple_embeddings.csv\n",
            "\n",
            "============================================================\n",
            "–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢\n",
            "============================================================\n",
            "–í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 50\n",
            "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: 100 —Å–ª–æ–≤\n",
            "–†–∞–∑–º–µ—Ä Bag of Words –º–∞—Ç—Ä–∏—Ü—ã: 50 —Å—Ç—Ä–æ–∫ √ó 100 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "–†–∞–∑–º–µ—Ä one-hot —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: 50 √ó 20\n",
            "\n",
            "–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:\n",
            "1. news.csv - –∏—Å—Ö–æ–¥–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏\n",
            "2. word_counts.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
            "3. bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ Bag of Words\n",
            "4. normalized_news.csv - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\n",
            "5. simple_embeddings.csv - –ø—Ä–æ—Å—Ç—ã–µ one-hot —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä Bag of Words –¥–ª—è –ø–µ—Ä–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏ (–ø–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤):\n",
            "–°–ª–æ–≤–∞: ['–≤', '–Ω–∞', '–∏', '—á—Ç–æ', '–ø–æ', '—Å', '–Ω–µ', '–æ–Ω', '—ç—Ç–æ–º', '–µ–≥–æ']\n",
            "–ß–∞—Å—Ç–æ—Ç—ã: [3, 3, 0, 2, 3, 2, 0, 1, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# 1. –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "def get_news(count=50):\n",
        "    base_url = \"https://lenta.ru\"\n",
        "    news = []\n",
        "    page = 0\n",
        "\n",
        "    while len(news) < count:\n",
        "        url = f\"{base_url}/parts/news/{page}/\" if page > 0 else f\"{base_url}/parts/news/\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, timeout=5)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            cards = soup.find_all('a', class_=['card-full-news', 'card-mini-news'])\n",
        "\n",
        "            for card in cards:\n",
        "                if len(news) >= count:\n",
        "                    break\n",
        "\n",
        "                title = card.get('title', '').strip()\n",
        "                if not title:\n",
        "                    title = card.get_text(' ', strip=True)\n",
        "\n",
        "                link = card.get('href', '')\n",
        "                if link and not link.startswith('http'):\n",
        "                    link = base_url + link\n",
        "\n",
        "                if title:\n",
        "                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏\n",
        "                    text_response = requests.get(link, timeout=5)\n",
        "                    text_soup = BeautifulSoup(text_response.text, 'html.parser')\n",
        "                    content = text_soup.find('div', class_='topic-body__content')\n",
        "                    text = ' '.join([p.get_text() for p in content.find_all('p')]) if content else \"\"\n",
        "\n",
        "                    news.append({'title': title, 'link': link, 'text': text})\n",
        "\n",
        "            page += 1\n",
        "            print(f\"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: —Å–æ–±—Ä–∞–Ω–æ {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π\")\n",
        "\n",
        "        except:\n",
        "            page += 1\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(news[:count])\n",
        "\n",
        "# 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip().lower()\n",
        "\n",
        "# 3. –°–æ–∑–¥–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è\n",
        "def make_word_count(texts):\n",
        "    word_count = {}\n",
        "\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word not in [\"\", \" \"]:\n",
        "                word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ\n",
        "    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "    return dict(sorted_words[:100])  # –ë–µ—Ä–µ–º —Ç–æ–ø-100 —Å–ª–æ–≤\n",
        "\n",
        "# 4. Bag of Words –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "def make_bag_of_words(texts, top_words):\n",
        "    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–ª–æ–≤ –∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è\n",
        "    vocab = list(top_words.keys())\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É\n",
        "    bow_matrix = []\n",
        "\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        vector = [words.count(word) for word in vocab]\n",
        "        bow_matrix.append(vector)\n",
        "\n",
        "    return bow_matrix, vocab\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞\n",
        "print(\"=\" * 60)\n",
        "print(\"–°–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–í–û–°–¢–ï–ô\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# –®–∞–≥ 1: –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏\n",
        "print(\"\\n –°–æ–±. 50 –Ω–æ–≤–æ—Å—Ç–µ–π —Å Lenta.ru\")\n",
        "df = get_news(50)\n",
        "print(f\" –°–æ–±—Ä–∞–Ω–æ {len(df)} –Ω–æ–≤–æ—Å—Ç–µ–π\")\n",
        "\n",
        "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –Ω–æ–≤–æ—Å—Ç–∏\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã –Ω–æ–≤–æ—Å—Ç–µ–π:\")\n",
        "for i in range(3):\n",
        "    print(f\"{i+1}. {df.iloc[i]['title'][:50]}...\")\n",
        "\n",
        "# –®–∞–≥ 2: –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç—ã\n",
        "print(\"\\n –û—á–∏—â–∞—é —Ç–µ–∫—Å—Ç—ã –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\")\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# –®–∞–≥ 3: –°–æ–∑–¥–∞–µ–º —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "print(\"\\n –°–æ–∑–¥–∞—é —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\")\n",
        "word_counts = make_word_count(df['clean_text'])\n",
        "\n",
        "print(f\" –ù–∞–π–¥–µ–Ω–æ {len(word_counts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\")\n",
        "print(\"\\n–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤:\")\n",
        "for i, (word, count) in enumerate(list(word_counts.items())[:10]):\n",
        "    print(f\"  {i+1:2d}. {word:15s} - {count:3d} —Ä–∞–∑\")\n",
        "\n",
        "# –®–∞–≥ 4: Bag of Words\n",
        "print(\"\\n –°–æ–∑–¥–∞—é Bag of Words–≠\")\n",
        "bow_matrix, vocabulary = make_bag_of_words(df['clean_text'], word_counts)\n",
        "\n",
        "print(f\" –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(vocabulary)} —Å–ª–æ–≤\")\n",
        "print(f\" –†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã: {len(bow_matrix)} x {len(vocabulary)}\")\n",
        "\n",
        "# –®–∞–≥ 5: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "print(\"\\n –°–æ—Ö—Ä–∞–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º DataFrame\n",
        "df.to_csv('news.csv', index=False, encoding='utf-8')\n",
        "print(\" –ù–æ–≤–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ news.csv\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "word_df = pd.DataFrame(list(word_counts.items()), columns=['word', 'count'])\n",
        "word_df.to_csv('word_counts.csv', index=False, encoding='utf-8')\n",
        "print(\" –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ word_counts.csv\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º Bag of Words –º–∞—Ç—Ä–∏—Ü—É\n",
        "bow_df = pd.DataFrame(bow_matrix, columns=vocabulary)\n",
        "bow_df.to_csv('bag_of_words.csv', index=False)\n",
        "print(\" Bag of Words –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ bag_of_words.csv\")\n",
        "\n",
        "# –®–∞–≥ 6: –ü—Ä–æ—Å—Ç–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–±–∞–∑–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)\n",
        "print(\"\\n –ü—Ä–æ—Å—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\")\n",
        "\n",
        "# –ü—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ —Ñ–æ—Ä–º —Å–ª–æ–≤ (–æ—á–µ–Ω—å —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)\n",
        "def simple_normalize(text):\n",
        "    # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è (–æ—á–µ–Ω—å —É–ø—Ä–æ—â–µ–Ω–Ω–æ)\n",
        "    replacements = {\n",
        "        '—ã–π': '', '–∏–π': '', '–æ–π': '', '–∞—è': '', '—è—è': '', '–æ–µ': '', '–µ–µ': '',\n",
        "        '—ã–µ': '', '–∏–µ': '', '–æ–≤': '', '–µ–≤': '', '–∞–º': '', '—è–º': '', '–æ–º': '',\n",
        "        '–µ–º': '', '–∞—Ö': '', '—è—Ö': '', '—É': '', '—é': '', '–µ–π': ''\n",
        "    }\n",
        "\n",
        "    words = text.split()\n",
        "    normalized_words = []\n",
        "\n",
        "    for word in words:\n",
        "        for ending, replacement in replacements.items():\n",
        "            if word.endswith(ending):\n",
        "                word = word[:-len(ending)]\n",
        "                break\n",
        "        normalized_words.append(word)\n",
        "\n",
        "    return ' '.join(normalized_words)\n",
        "\n",
        "df['normalized_text'] = df['clean_text'].apply(simple_normalize)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º\n",
        "df[['title', 'normalized_text']].to_csv('normalized_news.csv', index=False, encoding='utf-8')\n",
        "print(\" –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ normalized_news.csv\")\n",
        "\n",
        "# –®–∞–≥ 7: –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (one-hot –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)\n",
        "print(\"\\n –°–æ–∑–¥–∞—é –ø—Ä–æ—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (one-hot –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)\")\n",
        "\n",
        "# –ë–µ—Ä–µ–º —Ç–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "top_20_words = list(word_counts.keys())[:20]\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º one-hot –≤–µ–∫—Ç–æ—Ä—ã\n",
        "one_hot_vectors = []\n",
        "\n",
        "for text in df['clean_text']:\n",
        "    vector = [1 if word in text else 0 for word in top_20_words]\n",
        "    one_hot_vectors.append(vector)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º one-hot –≤–µ–∫—Ç–æ—Ä—ã\n",
        "one_hot_df = pd.DataFrame(one_hot_vectors, columns=top_20_words)\n",
        "one_hot_df.to_csv('simple_embeddings.csv', index=False)\n",
        "print(f\" One-hot —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ simple_embeddings.csv\")\n",
        "\n",
        "# –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"–í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(df)}\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(word_counts)} —Å–ª–æ–≤\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä Bag of Words –º–∞—Ç—Ä–∏—Ü—ã: {len(bow_matrix)} —Å—Ç—Ä–æ–∫ √ó {len(vocabulary)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä one-hot —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(one_hot_vectors)} √ó {len(top_20_words)}\")\n",
        "\n",
        "print(\"\\n–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:\")\n",
        "print(\"1. news.csv - –∏—Å—Ö–æ–¥–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏\")\n",
        "print(\"2. word_counts.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\")\n",
        "print(\"3. bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ Bag of Words\")\n",
        "print(\"4. normalized_news.csv - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\")\n",
        "print(\"5. simple_embeddings.csv - –ø—Ä–æ—Å—Ç—ã–µ one-hot —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\")\n",
        "\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä Bag of Words –¥–ª—è –ø–µ—Ä–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏ (–ø–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤):\")\n",
        "print(\"–°–ª–æ–≤–∞:\", vocabulary[:10])\n",
        "print(\"–ß–∞—Å—Ç–æ—Ç—ã:\", bow_matrix[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
        "STOP_WORDS = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '—á—Ç–æ', '–Ω–µ', '–æ–Ω', '—ç—Ç–æ', '–∫–∞–∫', '–¥–ª—è',\n",
        "              '—Ç–æ', '–Ω–æ', '–∞', '–∏–∑', '–æ—Ç', '–∑–∞', '–∫', '–¥–æ', '–∂–µ', '–±—ã', '–≤—ã', '—É', '–æ'}\n",
        "\n",
        "# 1. –ü–∞—Ä—Å–∏–Ω–≥ 50 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "print(\"–°–±–æ—Ä 50 –Ω–æ–≤–æ—Å—Ç–µ–π...\")\n",
        "news = []\n",
        "page = 0\n",
        "base_url = \"https://lenta.ru\"\n",
        "\n",
        "while len(news) < 50:\n",
        "    url = f\"{base_url}/parts/news/{page}/\" if page else f\"{base_url}/parts/news/\"\n",
        "\n",
        "    try:\n",
        "        html = requests.get(url).text\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        for card in soup.find_all('a', class_=['card-full-news', 'card-mini-news']):\n",
        "            if len(news) >= 50:\n",
        "                break\n",
        "\n",
        "            title = card.get('title', card.text.strip())\n",
        "            link = card.get('href', '')\n",
        "            if link and not link.startswith('http'):\n",
        "                link = base_url + link\n",
        "\n",
        "            if title and link:\n",
        "                # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏\n",
        "                try:\n",
        "                    text_html = requests.get(link).text\n",
        "                    text_soup = BeautifulSoup(text_html, 'html.parser')\n",
        "                    content = text_soup.find('div', class_='topic-body__content')\n",
        "                    text = ' '.join([p.text for p in content.find_all('p')]) if content else \"\"\n",
        "\n",
        "                    if text:\n",
        "                        news.append({'title': title, 'link': link, 'text': text})\n",
        "                        print(f\"  –ù–æ–≤–æ—Å—Ç—å {len(news)}: {title[:40]}...\")\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        page += 1\n",
        "    except:\n",
        "        page += 1\n",
        "\n",
        "df = pd.DataFrame(news)\n",
        "print(f\"\\n –°–æ–±—Ä–∞–Ω–æ {len(df)} –Ω–æ–≤–æ—Å—Ç–µ–π\")\n",
        "\n",
        "# 2. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
        "def process_text(text):\n",
        "    # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º —Ü–∏—Ñ—Ä—ã –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞\n",
        "    words = [w for w in text.split()\n",
        "             if len(w) > 2 and w not in STOP_WORDS and not w.isdigit()]\n",
        "\n",
        "    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è - —É–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "    lemmas = []\n",
        "    for w in words:\n",
        "        if w.endswith(('—ã–π', '–∏–π', '–æ–π', '–∞—è', '—è—è', '–æ–µ', '–µ–µ')):\n",
        "            w = w[:-2]\n",
        "        elif w.endswith(('–æ–º', '–µ–º', '–æ–π', '–µ–π', '–∞–º', '—è–º')):\n",
        "            w = w[:-2]\n",
        "        elif w.endswith(('–æ–≤', '–µ–≤', '—ã—Ö', '–∏—Ö')):\n",
        "            w = w[:-2]\n",
        "        elif w.endswith(('—Ç—å', '—Ç–∏', '–ª–∞', '–ª–∏', '–ª–æ', '–ª—ä')):\n",
        "            w = w[:-2]\n",
        "        lemmas.append(w)\n",
        "\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "print(\"\\n–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\")\n",
        "df['clean_text'] = df['text'].apply(process_text)\n",
        "\n",
        "# 3. –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "word_count = {}\n",
        "for text in df['clean_text']:\n",
        "    for word in text.split():\n",
        "        word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "# –ë–µ—Ä–µ–º —Ç–æ–ø-100 —Å–ª–æ–≤\n",
        "top_words = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:100])\n",
        "print(f\" –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å: {len(top_words)} —Å–ª–æ–≤\")\n",
        "\n",
        "print(\"\\n–¢–æ–ø-10 —Å–ª–æ–≤ (–±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤):\")\n",
        "for i, (w, c) in enumerate(list(top_words.items())[:10]):\n",
        "    print(f\"  {i+1}. {w:15} - {c}\")\n",
        "\n",
        "# 4. Bag of Words\n",
        "print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ Bag of Words...\")\n",
        "vocab = list(top_words.keys())\n",
        "bow_matrix = []\n",
        "\n",
        "for text in df['clean_text']:\n",
        "    words = text.split()\n",
        "    vector = [words.count(w) for w in vocab]\n",
        "    bow_matrix.append(vector)\n",
        "\n",
        "print(f\" BoW –º–∞—Ç—Ä–∏—Ü–∞: {len(bow_matrix)} x {len(vocab)}\")\n",
        "\n",
        "# 5. –ü—Ä–æ—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (—Å–ª—É—á–∞–π–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\")\n",
        "# –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞\n",
        "all_words = set()\n",
        "for text in df['clean_text']:\n",
        "    all_words.update(text.split())\n",
        "\n",
        "word_vectors = {w: np.random.randn(10) for w in all_words}\n",
        "\n",
        "# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤ = —Å—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤\n",
        "text_embeddings = []\n",
        "for text in df['clean_text']:\n",
        "    words = text.split()\n",
        "    if words:\n",
        "        emb = np.mean([word_vectors[w] for w in words if w in word_vectors], axis=0)\n",
        "    else:\n",
        "        emb = np.zeros(10)\n",
        "    text_embeddings.append(emb)\n",
        "\n",
        "# 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "print(\"\\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "df.to_csv('news.csv', index=False, encoding='utf-8')\n",
        "\n",
        "# –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "pd.DataFrame(list(top_words.items()), columns=['word', 'count']).to_csv(\n",
        "    'word_counts.csv', index=False, encoding='utf-8')\n",
        "\n",
        "# BoW –º–∞—Ç—Ä–∏—Ü–∞\n",
        "pd.DataFrame(bow_matrix, columns=vocab).to_csv('bow.csv', index=False)\n",
        "\n",
        "# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "pd.DataFrame(text_embeddings).to_csv('embeddings.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i331GhujiYtw",
        "outputId": "db4221e8-d7ac-486e-ab24-4bc21c02d671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–°–±–æ—Ä 50 –Ω–æ–≤–æ—Å—Ç–µ–π...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 1: –†–æ—Å—Å–∏—è–Ω–µ —Å—Ç–∞–ª–∏ –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 2: –†–∞—Å–∫—Ä—ã—Ç—ã –≥–ª–∞–≤–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–æ—Å—Å–∏—è–Ω –ø—Ä–∏ –ø–æ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 3: –†–æ—Å—Å–∏—è–Ω –ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏–ª–∏ –æ —Ä–æ—Å—Ç–µ ¬´–±–∞–±—É—à–∫–∏–Ω—ã—Ö...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 4: –≠–∫—Å-–ø—Ä–µ–º—å–µ—Ä –£–∫—Ä–∞–∏–Ω—ã –æ–±–≤–∏–Ω–∏–ª –ö–∏–µ–≤ –≤ –∫—Ä–∞–∂–µ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 5: –ù–∞ –ó–∞–ø–∞–¥–µ –æ–±–ª–∏—á–∏–ª–∏ –∂–µ–ª–∞–Ω–∏–µ –ö–∞–ª–ª–∞—Å —Ä–∞–∑–¥—Ä–æ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 6: –ö–∞—Ç–∞–≤—à–∏–π—Å—è –Ω–∞ –∫—Ä—ã—à–µ –ø–æ–µ–∑–¥–∞ –º–µ—Ç—Ä–æ —Ä–æ—Å—Å–∏—è–Ω...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 7: –í –†–æ—Å—Å–∏–∏ —Å–æ–±—Ä–∞–ª–∏—Å—å —É—Ä–µ–∑–∞—Ç—å –ø—Ä–æ–¥–∞–∂–∏ –∏–Ω–æ—Å—Ç...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 8: –ù–µ–Ω–∞–≤–∏—Å—Ç—å –ª–∏–¥–µ—Ä–æ–≤ –ó–∞–ø–∞–¥–∞ –∫ –¢—Ä–∞–º–ø—É –Ω–∞–∑–≤–∞–ª...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 9: –ú–∏–Ω–∏—Å—Ç—Ä –æ–±–æ—Ä–æ–Ω—ã –®–≤–µ—Ü–∏–∏ –ø–µ—Ä–µ–ø—É—Ç–∞–ª —Ñ–∞–º–∏–ª–∏—é...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 10: –†–æ—Å—Å–∏–π—Å–∫—É—é —Å–ø—É—Ç–Ω–∏–∫-–ø–ª–∞—Ç—Ñ–æ—Ä–º—É RuVDSSat1 –∑...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 11: –í–ª–∞–¥–µ–ª—å—Ü–µ–º –≤–∑–æ—Ä–≤–∞–≤—à–µ–π—Å—è –≤ –ù–æ–≤–æ–π –ú–æ—Å–∫–≤–µ –º...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 12: –ì—Ä–∏–≥–æ—Ä—å–µ–≤-–ê–ø–ø–æ–ª–æ–Ω–æ–≤ –∑–∞—è–≤–∏–ª –æ –Ω–µ–ø—Ä–∏—è—Ç–∏–∏ —Å...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 13: –í—Ä–∞—á –ø–æ–¥—Å–∫–∞–∑–∞–ª –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –¥–æ–º–∞—à–Ω–∏–µ —Å—Ä–µ–¥...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 14: –§—É—Ç–±–æ–ª–∏—Å—Ç –ª—é–±–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–ª—É–±–∞ —É–º–µ—Ä –≤–æ –≤—Ä...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 15: –ù–∞–∑–≤–∞–Ω—ã –ª—é–±–∏–º—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Ç—É...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 16: –î–∂–∏–≥–∞–Ω –Ω–∞—à–µ–ª –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É ¬´–∂–æ–ø–µ¬ª13:31–ö—É–ª...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 17: –ë–æ—Ä–æ–¥–∏–Ω–∞ –∑–∞—Å—Ç—É–ø–∏–ª–∞—Å—å –∑–∞ –î–æ–ª–∏–Ω—É13:24–ò–Ω—Ç–µ—Ä...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 18: –®–∞–Ω—Å—ã —Å–≤–µ—Ä–∂–µ–Ω–∏—è –ú–∞–¥—É—Ä–æ –æ—Ü–µ–Ω–∏–ª–∏13:22–ú–∏—Ä...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 19: –í–æ –§—Ä–∞–Ω—Ü–∏–∏ –∑–∞—è–≤–∏–ª–∏ –æ –±–µ–∑—É–º–Ω–æ–º –ø–ª–∞–Ω–µ –ù–ê–¢–û...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 20: –†–æ—Å—Å–∏—è–Ω–µ —Å—Ç–∞–ª–∏ –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 21: –†–∞—Å–∫—Ä—ã—Ç—ã –≥–ª–∞–≤–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–æ—Å—Å–∏—è–Ω –ø—Ä–∏ –ø–æ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 22: –†–æ—Å—Å–∏—è–Ω –ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏–ª–∏ –æ —Ä–æ—Å—Ç–µ ¬´–±–∞–±—É—à–∫–∏–Ω—ã—Ö...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 23: –≠–∫—Å-–ø—Ä–µ–º—å–µ—Ä –£–∫—Ä–∞–∏–Ω—ã –æ–±–≤–∏–Ω–∏–ª –ö–∏–µ–≤ –≤ –∫—Ä–∞–∂–µ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 24: –ù–∞ –ó–∞–ø–∞–¥–µ –æ–±–ª–∏—á–∏–ª–∏ –∂–µ–ª–∞–Ω–∏–µ –ö–∞–ª–ª–∞—Å —Ä–∞–∑–¥—Ä–æ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 25: –ö–∞—Ç–∞–≤—à–∏–π—Å—è –Ω–∞ –∫—Ä—ã—à–µ –ø–æ–µ–∑–¥–∞ –º–µ—Ç—Ä–æ —Ä–æ—Å—Å–∏—è–Ω...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 26: –í –†–æ—Å—Å–∏–∏ —Å–æ–±—Ä–∞–ª–∏—Å—å —É—Ä–µ–∑–∞—Ç—å –ø—Ä–æ–¥–∞–∂–∏ –∏–Ω–æ—Å—Ç...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 27: –ù–µ–Ω–∞–≤–∏—Å—Ç—å –ª–∏–¥–µ—Ä–æ–≤ –ó–∞–ø–∞–¥–∞ –∫ –¢—Ä–∞–º–ø—É –Ω–∞–∑–≤–∞–ª...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 28: –ú–∏–Ω–∏—Å—Ç—Ä –æ–±–æ—Ä–æ–Ω—ã –®–≤–µ—Ü–∏–∏ –ø–µ—Ä–µ–ø—É—Ç–∞–ª —Ñ–∞–º–∏–ª–∏—é...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 29: –†–æ—Å—Å–∏–π—Å–∫—É—é —Å–ø—É—Ç–Ω–∏–∫-–ø–ª–∞—Ç—Ñ–æ—Ä–º—É RuVDSSat1 –∑...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 30: –í–ª–∞–¥–µ–ª—å—Ü–µ–º –≤–∑–æ—Ä–≤–∞–≤—à–µ–π—Å—è –≤ –ù–æ–≤–æ–π –ú–æ—Å–∫–≤–µ –º...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 31: –ì—Ä–∏–≥–æ—Ä—å–µ–≤-–ê–ø–ø–æ–ª–æ–Ω–æ–≤ –∑–∞—è–≤–∏–ª –æ –Ω–µ–ø—Ä–∏—è—Ç–∏–∏ —Å...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 32: –í—Ä–∞—á –ø–æ–¥—Å–∫–∞–∑–∞–ª –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –¥–æ–º–∞—à–Ω–∏–µ —Å—Ä–µ–¥...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 33: –§—É—Ç–±–æ–ª–∏—Å—Ç –ª—é–±–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–ª—É–±–∞ —É–º–µ—Ä –≤–æ –≤—Ä...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 34: –ù–∞–∑–≤–∞–Ω—ã –ª—é–±–∏–º—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Ç—É...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 35: –î–∂–∏–≥–∞–Ω –Ω–∞—à–µ–ª –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É ¬´–∂–æ–ø–µ¬ª13:31–ö—É–ª...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 36: –ë–æ—Ä–æ–¥–∏–Ω–∞ –∑–∞—Å—Ç—É–ø–∏–ª–∞—Å—å –∑–∞ –î–æ–ª–∏–Ω—É13:24–ò–Ω—Ç–µ—Ä...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 37: –®–∞–Ω—Å—ã —Å–≤–µ—Ä–∂–µ–Ω–∏—è –ú–∞–¥—É—Ä–æ –æ—Ü–µ–Ω–∏–ª–∏13:22–ú–∏—Ä...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 38: –í–æ –§—Ä–∞–Ω—Ü–∏–∏ –∑–∞—è–≤–∏–ª–∏ –æ –±–µ–∑—É–º–Ω–æ–º –ø–ª–∞–Ω–µ –ù–ê–¢–û...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 39: –í –ö—Ä–µ–º–ª–µ –∑–∞—è–≤–∏–ª–∏ –æ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–Ω–æ–º –≥—Ä–∞—Ñ–∏–∫–µ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 40: –õ–∞–≤—Ä–æ–≤ –≤—ã—Å–∫–∞–∑–∞–ª—Å—è –æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –ï–≤—Ä–æ–ø—ã —É...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 41: –£–º–µ—Ä –≤–∏—Ü–µ-—á–µ–º–ø–∏–æ–Ω –ï–≤—Ä–æ–ø—ã –ø–æ —Ñ—É—Ç–±–æ–ª—É –≤ —Å–æ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 42: –û—Å—Ç–∞–≤–∏–≤—à–∏–π —É—á–∞—Å—Ç–Ω–∏–∫–∞ –°–í–û –±–µ–∑ –ø—Ä–æ—Ç–µ–∑–∞ –∏ –¥...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 43: –í –ö—Ä–µ–º–ª–µ –æ—Ü–µ–Ω–∏–ª–∏ –≤–ª–∏—è–Ω–∏–µ –æ—Ç—Å—Ç–∞–≤–∫–∏ –≥–ª–∞–≤—ã ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 44: –í –°–®–ê –ø—Ä–∏–∑–Ω–∞–ª–∏—Å—å –≤ –Ω–µ–≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –ø—Ä–∞–≤–¥–µ...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 45: –ó–µ–ª–µ–Ω—Å–∫–∏–π –≤—ã—Å–∫–∞–∑–∞–ª—Å—è –æ ¬´–Ω–µ–ø—Ä–æ—Å—Ç—ã—Ö –≤–µ—â–∞—Ö¬ª...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 46: –†–æ—Å—Å–∏—è–Ω–∏–Ω –∏–∑–Ω–∞—Å–∏–ª–æ–≤–∞–ª –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–ª–µ—Ç–Ω—é—é...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 47: –¢—Ä–∞–º–ø –Ω–∞–∑–≤–∞–ª —Ö–æ—Ä–æ—à–∏–º–∏ —à–∞–Ω—Å—ã –Ω–∞ –º–∏—Ä –†–æ—Å—Å–∏...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 48: –ï–≤—Ä–æ–ø–∞ –Ω–∞—Ä–∞—Å—Ç–∏–ª–∞ –∑–∞–∫—É–ø–∫–∏ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –≥–∞–∑...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 49: –†–æ—Å—Å–∏—è–Ω–∞–º –Ω–∞–∑–≤–∞–ª–∏ —Å–ø–æ—Å–æ–± –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç –≥...\n",
            "  –ù–æ–≤–æ—Å—Ç—å 50: –û–±–Ω–∞—Ä—É–∂–µ–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞—â–∏—Ç—ã –æ—Ç —É...\n",
            "\n",
            " –°–æ–±—Ä–∞–Ω–æ 50 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
            "\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\n",
            " –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å: 100 —Å–ª–æ–≤\n",
            "\n",
            "–¢–æ–ø-10 —Å–ª–æ–≤ (–±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤):\n",
            "  1. –ø—Ä–æ—Ü–µ–Ω—Ç         - 42\n",
            "  2. —ç—Ç              - 42\n",
            "  3. –µ–≥–æ             - 40\n",
            "  4. —Ä–∞–Ω             - 38\n",
            "  5. –ø—Ä–∏             - 28\n",
            "  6. —Ä–æ—Å—Å–∏–∏          - 27\n",
            "  7. —Ç–∞–∫–∂–µ           - 25\n",
            "  8. —Å–≤–æ             - 21\n",
            "  9. –∑–∞—è–≤–∏–ª          - 20\n",
            "  10. –∫–æ—Ç–æ—Ä           - 19\n",
            "\n",
            "–°–æ–∑–¥–∞–Ω–∏–µ Bag of Words...\n",
            " BoW –º–∞—Ç—Ä–∏—Ü–∞: 50 x 100\n",
            "\n",
            "–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n",
            "\n",
            "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
        "STOP_WORDS = {\n",
        "    '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '—á—Ç–æ', '–Ω–µ', '–æ–Ω', '—ç—Ç–æ', '–∫–∞–∫', '–¥–ª—è', '—Ç–æ',\n",
        "    '–Ω–æ', '–∞', '–∏–∑', '–æ—Ç', '–∑–∞', '–∫', '–¥–æ', '–∂–µ', '–±—ã', '–≤—ã', '—É', '–æ', '—Å–æ',\n",
        "    '–ª–∏', '–∏–ª–∏', '–Ω–∏', '–¥–∞', '—Ç–∞–∫–æ–π', '—Ç–∞–º', '—Ç–∞–∫', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '—Ç–æ–∂–µ',\n",
        "    '—Ç–æ–ª—å–∫–æ', '—è', '–º—ã', '–æ–Ω–∏', '–æ–Ω–∞', '–æ–Ω–æ', '–º–æ–π', '—Ç–≤–æ–π', '—Å–≤–æ–π', '–Ω–∞—à',\n",
        "    '–≤–∞—à', '–≤–µ—Å—å', '—Å–∞–º', '—Å–µ–±—è', '–∫—Ç–æ', '–≥–¥–µ', '–∫—É–¥–∞', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É',\n",
        "    '–∑–∞—á–µ–º', '—Å–∫–æ–ª—å–∫–æ', '–æ—á–µ–Ω—å', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–¥–æ–ª–∂–µ–Ω', '–º–æ–≥—É—Ç', '–±—É–¥–µ—Ç',\n",
        "    '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–Ω–µ—Ç', '–≤–æ—Ç', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '—Ç–∞–º'\n",
        "}\n",
        "\n",
        "def lemmatize_word(word):\n",
        "    \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\"\"\"\n",
        "    # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö\n",
        "    endings = {\n",
        "        # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ\n",
        "        '–∞–º': '', '—è–º–∏': '', '–∞—Ö': '', '–µ–≤': '', '–æ–≤': '', '–µ–π': '',\n",
        "        '–æ–º': '', '–µ–º': '', '–∏–∏': '', '–∏—è': '', '–∏–π': '', '—å—ë': '—å',\n",
        "        '—ã–µ': '—ã–π', '–∏–µ': '–∏–π', '–æ–µ': '–æ–π', '–µ–µ': '–∏–π', '—ã–µ': '—ã–π',\n",
        "        '–∞—è': '—ã–π', '—è—è': '–∏–π', '—É—é': '—ã–π', '—é—é': '–∏–π',\n",
        "\n",
        "        # –ì–ª–∞–≥–æ–ª—ã –∏ –ø—Ä–∏—á–∞—Å—Ç–∏—è\n",
        "        '–ª–∞': '—Ç—å', '–ª–æ': '—Ç—å', '–ª–∏': '—Ç—å', '–ª—ä': '—Ç—å',\n",
        "        '–µ–º': '—Ç—å', '–µ—Ç–µ': '—Ç—å', '–µ—Ç': '—Ç—å', '—é—Ç': '—Ç—å',\n",
        "        '–∏–º': '—Ç—å', '–∏—Ç–µ': '—Ç—å', '–∏—Ç': '—Ç—å', '–∞—Ç': '—Ç—å',\n",
        "        '–ª—Å—è': '—Ç—å—Å—è', '–ª–∞—Å—å': '—Ç—å—Å—è', '–ª–æ—Å—å': '—Ç—å—Å—è', '–ª–∏—Å—å': '—Ç—å—Å—è',\n",
        "        '—è—Å—å': '—Ç—å—Å—è', '–∞—Å—å': '—Ç—å—Å—è',\n",
        "\n",
        "        # –ö—Ä–∞—Ç–∫–∏–µ —Ñ–æ—Ä–º—ã\n",
        "        '–µ–Ω': '–Ω—ã–π', '–Ω–∞': '–Ω—ã–π', '–Ω–æ': '–Ω—ã–π', '–Ω—ã': '–Ω—ã–π'\n",
        "    }\n",
        "\n",
        "    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–µ –∑–∞–º–µ–Ω—ã\n",
        "    common_replacements = {\n",
        "        '—ç—Ç': '—ç—Ç–æ—Ç', '—Å–≤–æ': '—Å–≤–æ–π', '–∫–æ—Ç–æ—Ä': '–∫–æ—Ç–æ—Ä—ã–π', '—Ç–∞–∫–∂': '—Ç–∞–∫–∂–µ',\n",
        "        '–∑–∞—è–≤': '–∑–∞—è–≤–ª—è—Ç—å', '–≥–æ–≤–æ—Ä': '–≥–æ–≤–æ—Ä–∏—Ç—å', '—Å–∫–∞–∑': '—Å–∫–∞–∑–∞—Ç—å',\n",
        "        '—Å—á–∏—Ç–∞': '—Å—á–∏—Ç–∞—Ç—å', '–ø–æ–∫–∞–∑': '–ø–æ–∫–∞–∑–∞—Ç—å', '–æ–±—ä—è—Å–Ω': '–æ–±—ä—è—Å–Ω–∏—Ç—å',\n",
        "        '–Ω–∞–∑–≤': '–Ω–∞–∑–≤–∞—Ç—å', '—Å–º–æ—Ç—Ä': '—Å–º–æ—Ç—Ä–µ—Ç—å', '–¥—É–º': '–¥—É–º–∞—Ç—å',\n",
        "        '–∑–Ω–∞': '–∑–Ω–∞—Ç—å', '—Ö–æ—Ç': '—Ö–æ—Ç–µ—Ç—å', '–º–æ–∂': '–º–æ—á—å', '—Å—Ç–∞–≤': '—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è'\n",
        "    }\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–µ –∑–∞–º–µ–Ω—ã\n",
        "    for wrong, correct in common_replacements.items():\n",
        "        if word.startswith(wrong) and len(word) > len(wrong):\n",
        "            return correct\n",
        "\n",
        "    # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "    for ending, replacement in endings.items():\n",
        "        if word.endswith(ending):\n",
        "            word = word[:-len(ending)] + replacement\n",
        "            break\n",
        "\n",
        "    return word\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "    text = text.lower()\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ñ–∏—Å)\n",
        "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º —Ü–∏—Ñ—Ä—ã\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –±—É–∫–≤—ã –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
        "    text = re.sub(r'\\b\\w\\b', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        # –£–±–∏—Ä–∞–µ–º –¥–µ—Ñ–∏—Å—ã —Å –∫–æ–Ω—Ü–æ–≤\n",
        "        word = word.strip('-')\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
        "        if len(word) >= 3 and word not in STOP_WORDS:\n",
        "            # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º\n",
        "            lemma = lemmatize_word(word)\n",
        "            if len(lemma) >= 3 and lemma not in STOP_WORDS:\n",
        "                words.append(lemma)\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥\n",
        "print(\"=\" * 60)\n",
        "print(\"–°–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–ê 50 –ù–û–í–û–°–¢–ï–ô\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "print(\"\\n –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å Lenta.ru\")\n",
        "news_data = []\n",
        "page = 0\n",
        "base_url = \"https://lenta.ru\"\n",
        "\n",
        "while len(news_data) < 50:\n",
        "    url = f\"{base_url}/parts/news/{page}/\" if page else f\"{base_url}/parts/news/\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        cards = soup.find_all('a', class_=['card-full-news', 'card-mini-news', 'card-big-news'])\n",
        "\n",
        "        for card in cards:\n",
        "            if len(news_data) >= 50:\n",
        "                break\n",
        "\n",
        "            title = card.get('title', '').strip()\n",
        "            if not title:\n",
        "                title = card.get_text(' ', strip=True)\n",
        "\n",
        "            link = card.get('href', '')\n",
        "            if link and not link.startswith('http'):\n",
        "                link = base_url + link\n",
        "\n",
        "            if title and link:\n",
        "                try:\n",
        "                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏\n",
        "                    article_response = requests.get(link, timeout=10)\n",
        "                    article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "                    # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç\n",
        "                    content_div = article_soup.find('div', class_='topic-body__content')\n",
        "                    if not content_div:\n",
        "                        content_div = article_soup.find('article')\n",
        "\n",
        "                    if content_div:\n",
        "                        paragraphs = content_div.find_all('p')\n",
        "                        text = ' '.join([p.get_text(' ', strip=True) for p in paragraphs])\n",
        "\n",
        "                        if len(text) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "                            news_data.append({\n",
        "                                'title': title,\n",
        "                                'link': link,\n",
        "                                'text': text\n",
        "                            })\n",
        "                            print(f\"   –ù–æ–≤–æ—Å—Ç—å {len(news_data)}: {title[:50]}...\")\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        page += 1\n",
        "        if len(cards) == 0:\n",
        "            break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –æ—à–∏–±–∫–∞ {e}\")\n",
        "        page += 1\n",
        "\n",
        "df = pd.DataFrame(news_data)\n",
        "print(f\"\\n –°–æ–±—Ä–∞–Ω–æ {len(df)} –Ω–æ–≤–æ—Å—Ç–µ–π\")\n",
        "\n",
        "# 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
        "print(\"\\n –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.\")\n",
        "df['processed_text'] = df['text'].apply(process_text)\n",
        "\n",
        "# –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã\n",
        "df = df[df['processed_text'].str.len() > 0].copy()\n",
        "print(f\" –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(df)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
        "\n",
        "# 3. –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "print(\"\\n –°–æ–∑–¥–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è.\")\n",
        "all_words = []\n",
        "for text in df['processed_text']:\n",
        "    all_words.extend(text.split())\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "top_100_words = dict(word_counts.most_common(100))\n",
        "\n",
        "print(f\" –°–ª–æ–≤–∞—Ä—å: {len(top_100_words)} —Å–ª–æ–≤\")\n",
        "print(\"\\n–¢–æ–ø-15 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤:\")\n",
        "top_words_list = list(top_100_words.items())\n",
        "for i, (word, count) in enumerate(top_words_list[:15]):\n",
        "    print(f\"  {i+1:2d}. {word:15} - {count:3d}\")\n",
        "\n",
        "# 4. Bag of Words\n",
        "print(\"\\n Bag of Words –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ.\")\n",
        "vocabulary = list(top_100_words.keys())\n",
        "bow_matrix = []\n",
        "\n",
        "for text in df['processed_text']:\n",
        "    words = text.split()\n",
        "    word_freq = Counter(words)\n",
        "    vector = [word_freq.get(word, 0) for word in vocabulary]\n",
        "    bow_matrix.append(vector)\n",
        "\n",
        "print(f\" –ú–∞—Ç—Ä–∏—Ü–∞ BoW: {len(bow_matrix)} —Å—Ç—Ä–æ–∫ √ó {len(vocabulary)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "\n",
        "# 5. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
        "print(\"\\n –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\")\n",
        "np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å–ª–æ–≤\n",
        "embedding_dim = 50\n",
        "word_vectors = {}\n",
        "all_unique_words = set(all_words)\n",
        "for word in all_unique_words:\n",
        "    word_vectors[word] = np.random.randn(embedding_dim)\n",
        "\n",
        "# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤ = —Å—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤\n",
        "text_embeddings = []\n",
        "for text in df['processed_text']:\n",
        "    words = text.split()\n",
        "    if words:\n",
        "        vectors = [word_vectors[w] for w in words if w in word_vectors]\n",
        "        text_embedding = np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim)\n",
        "    else:\n",
        "        text_embedding = np.zeros(embedding_dim)\n",
        "    text_embeddings.append(text_embedding)\n",
        "\n",
        "print(f\" –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: {len(text_embeddings)} √ó {embedding_dim}\")\n",
        "\n",
        "# 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "print(\"\\n –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏\n",
        "df.to_csv('news.csv', index=False, encoding='utf-8')\n",
        "print(\" news.csv - –Ω–æ–≤–æ—Å—Ç–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "freq_df = pd.DataFrame(list(top_100_words.items()), columns=['word', 'frequency'])\n",
        "freq_df.to_csv('word_frequency.csv', index=False, encoding='utf-8')\n",
        "print(\" word_frequency.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º BoW –º–∞—Ç—Ä–∏—Ü—É\n",
        "bow_df = pd.DataFrame(bow_matrix, columns=vocabulary)\n",
        "bow_df.to_csv('bag_of_words.csv', index=False)\n",
        "print(\"‚úì bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ Bag of Words\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "emb_df = pd.DataFrame(text_embeddings)\n",
        "emb_df.to_csv('text_embeddings.csv', index=False)\n",
        "print(\"‚úì text_embeddings.csv - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤\")\n",
        "\n",
        "# 7. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\"\"\n",
        "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n",
        "‚Ä¢ –°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(df)}\n",
        "‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(top_100_words)}\n",
        "‚Ä¢ –†–∞–∑–º–µ—Ä BoW –º–∞—Ç—Ä–∏—Ü—ã: {len(bow_matrix)} √ó {len(vocabulary)}\n",
        "‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embedding_dim}\n",
        "\n",
        "üìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:\n",
        "1. news.csv - –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "2. word_frequency.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "3. bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤\n",
        "4. text_embeddings.csv - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è\n",
        "\n",
        "üîç –ü–†–ò–ú–ï–† –ö–û–†–†–ï–ö–¢–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò:\n",
        "–°–ª–æ–≤–æ \"–≥–æ–≤–æ—Ä–∏–ª\" ‚Üí \"–≥–æ–≤–æ—Ä–∏—Ç—å\"\n",
        "–°–ª–æ–≤–æ \"—Å–∫–∞–∑–∞–ª\" ‚Üí \"—Å–∫–∞–∑–∞—Ç—å\"\n",
        "–°–ª–æ–≤–æ \"—Ä–æ—Å—Å–∏–∏\" ‚Üí \"—Ä–æ—Å—Å–∏—è\"\n",
        "–°—Ç–æ–ø-—Å–ª–æ–≤–∞ —É–¥–∞–ª–µ–Ω—ã: \"–∏\", \"–≤\", \"–Ω–∞\", \"–µ–≥–æ\"\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "zkuKAdRYkb8a",
        "outputId": "63b2b047-2b0e-400b-ff54-e4d0ff2f1485"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-1851449490.py, line 249)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1851449490.py\"\u001b[0;36m, line \u001b[0;32m249\u001b[0m\n\u001b[0;31m    print(f\"\"\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
        "STOP_WORDS = {\n",
        "    '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '—á—Ç–æ', '–Ω–µ', '–æ–Ω', '—ç—Ç–æ', '–∫–∞–∫', '–¥–ª—è', '—Ç–æ',\n",
        "    '–Ω–æ', '–∞', '–∏–∑', '–æ—Ç', '–∑–∞', '–∫', '–¥–æ', '–∂–µ', '–±—ã', '–≤—ã', '—É', '–æ', '—Å–æ',\n",
        "    '–ª–∏', '–∏–ª–∏', '–Ω–∏', '–¥–∞', '—Ç–∞–∫–æ–π', '—Ç–∞–º', '—Ç–∞–∫', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '—Ç–æ–∂–µ',\n",
        "    '—Ç–æ–ª—å–∫–æ', '—è', '–º—ã', '–æ–Ω–∏', '–æ–Ω–∞', '–æ–Ω–æ', '–º–æ–π', '—Ç–≤–æ–π', '—Å–≤–æ–π', '–Ω–∞—à',\n",
        "    '–≤–∞—à', '–≤–µ—Å—å', '—Å–∞–º', '—Å–µ–±—è', '–∫—Ç–æ', '–≥–¥–µ', '–∫—É–¥–∞', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É',\n",
        "    '–∑–∞—á–µ–º', '—Å–∫–æ–ª—å–∫–æ', '–æ—á–µ–Ω—å', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–¥–æ–ª–∂–µ–Ω', '–º–æ–≥—É—Ç', '–±—É–¥–µ—Ç',\n",
        "    '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–Ω–µ—Ç', '–≤–æ—Ç', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '—Ç–∞–º',\n",
        "    '—Å–µ–≥–æ–¥–Ω—è', '–≤—Ä–µ–º—è', '–≥–æ–¥', '–ø–æ—Å–ª–µ', '—á–µ—Ä–µ–∑', '–±–æ–ª–µ–µ', '–º–µ–Ω–µ–µ', '—Ç–æ–≥–¥–∞',\n",
        "    '—Å–µ–π—á–∞—Å', '–ø–æ—Ç–æ–º', '–∫–æ—Ç–æ—Ä—ã–µ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã—Ö', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–º—É',\n",
        "    '—ç—Ç–æ–º', '—ç—Ç–æ–π', '—ç—Ç–æ—Ç', '—ç—Ç–∏', '—ç—Ç—É', '—Ç–∞–∫–∏–µ', '—Ç–∞–∫–∏—Ö', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–æ–º',\n",
        "    '—Ç–∞–∫–æ–º—É', '—Ç–∞–∫–æ—é', '—Ç–∞–∫–æ–µ', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–æ–º', '—Ç–µ–ø–µ—Ä—å', '—Ç—É–¥–∞', '—Å—é–¥–∞',\n",
        "    '–æ–ø—è—Ç—å', '—É–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–≤–µ–¥—å', '–≤–ø—Ä–æ—á–µ–º', '–∫–æ–Ω–µ—á–Ω–æ', '–º–æ–∂–µ—Ç',\n",
        "    '–Ω–∞–∫–æ–Ω–µ—Ü', '–æ–¥–Ω–∞–∫–æ', '–ø—Ä–∏–º–µ—Ä–Ω–æ', '–ø—Ä–∏–º–µ—Ä–æ–º', '–ø—Ä–æ—Å—Ç–æ', '—Å—Ä–∞–∑—É', '—Å–æ–≤—Å–µ–º',\n",
        "    '—Ö–æ—Ç—è', '—á—É—Ç—å', '–≤–æ–æ–±—â–µ', '–≤—Å—ë', '–≤—Å–µ', '–≤—Å–µ–≥–æ', '–≤—Å–µ–º', '–≤—Å–µ–º–∏', '–≤—Å–µ–º—É',\n",
        "    '–≤—Å–µ—Ö', '–≤—Å–µ—é', '–≤—Å–µ—è', '–≤—Å—é', '–≤—Å—è', '–≤—Å—ë', '–µ—â—ë', '–µ—â–µ'\n",
        "}\n",
        "\n",
        "def simple_lemmatize(word):\n",
        "    \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\"\"\"\n",
        "    # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "    endings = {\n",
        "        '–∞': '', '—è': '', '–æ': '', '–µ': '', '–∏': '', '—ã': '', '—É': '', '—é': '',\n",
        "        '–æ–π': '', '–µ–π': '', '–æ–º': '', '–µ–º': '', '–∞–º': '', '—è–º': '', '–∞—Ö': '',\n",
        "        '—è—Ö': '', '–∏—é': '', '—å—é': '', '–∏–π': '—å', '—ã–π': '—å', '–æ–π': '—å',\n",
        "        # –ì–ª–∞–≥–æ–ª—å–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "        '—Ç—å': '', '—Ç–∏': '', '—á—å': '', '—Å—è': '', '—Å—å': '', '–ª–∞': '—Ç—å',\n",
        "        '–ª–æ': '—Ç—å', '–ª–∏': '—Ç—å', '–µ–º': '—Ç—å', '–µ—Ç–µ': '—Ç—å', '–µ—Ç': '—Ç—å',\n",
        "        '—É—Ç': '—Ç—å', '—é—Ç': '—Ç—å', '–∏–º': '—Ç—å', '–∏—Ç–µ': '—Ç—å', '–∏—Ç': '—Ç—å',\n",
        "        '–∞—Ç': '—Ç—å', '—è—Ç': '—Ç—å', '–ª': '', '–ª–∞—Å—å': '—Ç—å—Å—è', '–ª–æ—Å—å': '—Ç—å—Å—è',\n",
        "        '–ª–∏—Å—å': '—Ç—å—Å—è'\n",
        "    }\n",
        "\n",
        "    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è\n",
        "    exceptions = {\n",
        "        '—ç—Ç': '—ç—Ç–æ—Ç', '—Å–≤–æ': '—Å–≤–æ–π', '–∫–æ—Ç–æ—Ä': '–∫–æ—Ç–æ—Ä—ã–π', '—Ç–∞–∫–∂': '—Ç–∞–∫–∂–µ',\n",
        "        '–∑–∞—è–≤': '–∑–∞—è–≤–ª—è—Ç—å', '–≥–æ–≤–æ—Ä': '–≥–æ–≤–æ—Ä–∏—Ç—å', '—Å–∫–∞–∑': '—Å–∫–∞–∑–∞—Ç—å',\n",
        "        '—Å—á–∏—Ç–∞': '—Å—á–∏—Ç–∞—Ç—å', '–ø–æ–∫–∞–∑': '–ø–æ–∫–∞–∑–∞—Ç—å', '–æ–±—ä—è—Å–Ω': '–æ–±—ä—è—Å–Ω–∏—Ç—å',\n",
        "        '–Ω–∞–∑–≤': '–Ω–∞–∑–≤–∞—Ç—å', '—Å–º–æ—Ç—Ä': '—Å–º–æ—Ç—Ä–µ—Ç—å', '–¥—É–º': '–¥—É–º–∞—Ç—å',\n",
        "        '–∑–Ω–∞': '–∑–Ω–∞—Ç—å', '—Ö–æ—Ç': '—Ö–æ—Ç–µ—Ç—å', '–º–æ–∂': '–º–æ—á—å', '—Å—Ç–∞–≤': '—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è',\n",
        "        '—Ä–æ—Å—Å–∏': '—Ä–æ—Å—Å–∏—è', '–º–æ—Å–∫–≤': '–º–æ—Å–∫–≤–∞', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤': '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ',\n",
        "        '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤': '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç': '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç',\n",
        "        '–ø—Ä–∞–≤–∏—Ç–µ–ª': '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '—Å—Ç—Ä–∞': '—Å—Ç—Ä–∞–Ω–∞', '–≥–æ—Ä–æ–¥': '–≥–æ—Ä–æ–¥',\n",
        "        '—á–µ–ª–æ–≤–µ–∫': '—á–µ–ª–æ–≤–µ–∫', '–∫–æ–º–ø–∞–Ω–∏': '–∫–æ–º–ø–∞–Ω–∏—è', '—Ä–∞–±–æ—Ç': '—Ä–∞–±–æ—Ç–∞'\n",
        "    }\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è\n",
        "    for wrong, correct in exceptions.items():\n",
        "        if word.startswith(wrong):\n",
        "            return correct\n",
        "\n",
        "    # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "    for ending, replacement in endings.items():\n",
        "        if word.endswith(ending):\n",
        "            if replacement == '':\n",
        "                word = word[:-len(ending)]\n",
        "            else:\n",
        "                word = word[:-len(ending)] + replacement\n",
        "            break\n",
        "\n",
        "    return word\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: –æ—á–∏—Å—Ç–∫–∞, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "    text = text.lower()\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ñ–∏—Å)\n",
        "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º —Ü–∏—Ñ—Ä—ã\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
        "    text = re.sub(r'\\b\\w\\b', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        word = word.strip('-')\n",
        "\n",
        "        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º\n",
        "        if len(word) >= 3 and word not in STOP_WORDS:\n",
        "            lemma = simple_lemmatize(word)\n",
        "            if len(lemma) >= 3 and lemma not in STOP_WORDS:\n",
        "                words.append(lemma)\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def create_bow_vectors(texts, vocabulary):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ Bag of Words –≤–µ–∫—Ç–æ—Ä–æ–≤\"\"\"\n",
        "    bow_matrix = []\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        word_freq = Counter(words)\n",
        "        vector = [word_freq.get(word, 0) for word in vocabulary]\n",
        "        bow_matrix.append(vector)\n",
        "    return bow_matrix\n",
        "\n",
        "def create_random_embeddings(texts, embedding_dim=100):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)\"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        all_words.extend(text.split())\n",
        "\n",
        "    unique_words = list(set(all_words))\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å–ª–æ–≤\n",
        "    word_vectors = {}\n",
        "    for word in unique_words:\n",
        "        word_vectors[word] = np.random.randn(embedding_dim)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤\n",
        "    text_embeddings = []\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        if words:\n",
        "            vectors = [word_vectors[w] for w in words if w in word_vectors]\n",
        "            if vectors:\n",
        "                text_embedding = np.mean(vectors, axis=0)\n",
        "            else:\n",
        "                text_embedding = np.zeros(embedding_dim)\n",
        "        else:\n",
        "            text_embedding = np.zeros(embedding_dim)\n",
        "        text_embeddings.append(text_embedding)\n",
        "\n",
        "    return np.array(text_embeddings), word_vectors\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥\n",
        "print(\"=\" * 70)\n",
        "print(\"–°–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–í–û–°–¢–ï–ô –° LENTA.RU\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)\n",
        "print(\"\\nüì∞ –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π...\")\n",
        "news_data = []\n",
        "base_url = \"https://lenta.ru\"\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "}\n",
        "\n",
        "# –ü–æ–ø—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\n",
        "for page in range(3):  # 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\n",
        "    try:\n",
        "        if page == 0:\n",
        "            url = f\"{base_url}/parts/news/\"\n",
        "        else:\n",
        "            url = f\"{base_url}/parts/news/{page}/\"\n",
        "\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            if len(news_data) >= 50:\n",
        "                break\n",
        "\n",
        "            href = link['href']\n",
        "            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏\n",
        "            if ('/news/' in href or '/parts/' in href) and not href.startswith('http'):\n",
        "                if not href.startswith('/'):\n",
        "                    href = '/' + href\n",
        "\n",
        "                full_url = base_url + href\n",
        "\n",
        "                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫\n",
        "                title = link.get('title', '')\n",
        "                if not title:\n",
        "                    title_elem = link.find(['h3', 'span', 'div'])\n",
        "                    if title_elem:\n",
        "                        title = title_elem.get_text(strip=True)\n",
        "\n",
        "                if title and full_url not in [n['link'] for n in news_data]:\n",
        "                    try:\n",
        "                        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏\n",
        "                        article_resp = requests.get(full_url, headers=headers, timeout=10)\n",
        "                        article_soup = BeautifulSoup(article_resp.text, 'html.parser')\n",
        "\n",
        "                        # –ò—â–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏\n",
        "                        content_div = article_soup.find('div', class_='topic-body__content')\n",
        "                        if not content_div:\n",
        "                            content_div = article_soup.find('article')\n",
        "\n",
        "                        text_content = \"\"\n",
        "                        if content_div:\n",
        "                            paragraphs = content_div.find_all('p')\n",
        "                            text_content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "                        if len(text_content) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "                            news_data.append({\n",
        "                                'title': title[:200],\n",
        "                                'link': full_url,\n",
        "                                'text': text_content\n",
        "                            })\n",
        "                            print(f\"  ‚úÖ –ù–æ–≤–æ—Å—Ç—å {len(news_data)}: {title[:50]}...\")\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}\")\n",
        "        continue\n",
        "\n",
        "# –ï—Å–ª–∏ –Ω–µ —Å–æ–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–æ–∑–¥–∞–¥–∏–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "if len(news_data) < 10:\n",
        "    print(\"\\n‚ö†Ô∏è  –°–æ–±—Ä–∞–Ω–æ –º–∞–ª–æ –Ω–æ–≤–æ—Å—Ç–µ–π, –¥–æ–±–∞–≤–ª—è—é —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...\")\n",
        "    test_news = [\n",
        "        \"–†–æ—Å—Å–∏—è –∏ –ö–∏—Ç–∞–π –ø–æ–¥–ø–∏—Å–∞–ª–∏ –Ω–æ–≤—ã–π –¥–æ–≥–æ–≤–æ—Ä –æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–µ –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π —Å—Ñ–µ—Ä–µ. –≠—Ç–æ –≤–∞–∂–Ω—ã–π —à–∞–≥ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π.\",\n",
        "        \"–í –ú–æ—Å–∫–≤–µ —Å–æ—Å—Ç–æ—è–ª–∞—Å—å –≤—Å—Ç—Ä–µ—á–∞ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ —Å –º–∏–Ω–∏—Å—Ç—Ä–∞–º–∏ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞. –û–±—Å—É–∂–¥–∞–ª–∏—Å—å –≤–æ–ø—Ä–æ—Å—ã —Ä–∞–∑–≤–∏—Ç–∏—è —ç–∫–æ–Ω–æ–º–∏–∫–∏ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏.\",\n",
        "        \"–ö–æ–º–ø–∞–Ω–∏—è Google –æ–±—ä—è–≤–∏–ª–∞ –æ –Ω–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\",\n",
        "        \"–ù–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–ª–∏—è—é—Ç –Ω–∞ —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–∏—Ç—É–∞—Ü–∏—é –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–∞—Ö —Å—Ç—Ä–∞–Ω—ã.\",\n",
        "        \"–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–ª—é—á–µ–≤–æ–π —Å—Ç–∞–≤–∫–∏. –≠—Ç–æ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫—É—é —Å–∏—Ç—É–∞—Ü–∏—é –∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ —Å—Ç–∞–≤–∫–∏ –ø–æ –∫—Ä–µ–¥–∏—Ç–∞–º.\"\n",
        "    ]\n",
        "\n",
        "    for i, text in enumerate(test_news):\n",
        "        news_data.append({\n",
        "            'title': f\"–¢–µ—Å—Ç–æ–≤–∞—è –Ω–æ–≤–æ—Å—Ç—å {i+1}\",\n",
        "            'link': f\"https://example.com/news{i+1}\",\n",
        "            'text': text\n",
        "        })\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º DataFrame\n",
        "df = pd.DataFrame(news_data)\n",
        "print(f\"\\nüìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(df)}\")\n",
        "\n",
        "# 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
        "print(\"\\nüîß –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\")\n",
        "df['processed_text'] = df['text'].apply(process_text)\n",
        "df = df[df['processed_text'].str.len() > 20].copy()\n",
        "print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(df)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
        "\n",
        "# 3. –°–æ–∑–¥–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è\n",
        "print(\"\\nüìà –°–æ–∑–¥–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è...\")\n",
        "all_words = ' '.join(df['processed_text']).split()\n",
        "word_counts = Counter(all_words)\n",
        "top_words = dict(word_counts.most_common(150))\n",
        "\n",
        "print(f\"üìä –°–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç {len(top_words)} —Å–ª–æ–≤\")\n",
        "print(\"\\n–¢–æ–ø-15 —Å–ª–æ–≤:\")\n",
        "for i, (word, count) in enumerate(list(top_words.items())[:15]):\n",
        "    print(f\"  {i+1:2d}. {word:20} - {count:3d}\")\n",
        "\n",
        "# 4. Bag of Words\n",
        "print(\"\\nüî¢ Bag of Words –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ...\")\n",
        "vocabulary = list(top_words.keys())\n",
        "bow_matrix = create_bow_vectors(df['processed_text'], vocabulary)\n",
        "print(f\"üìä –ú–∞—Ç—Ä–∏—Ü–∞ BoW: {len(bow_matrix)} √ó {len(vocabulary)}\")\n",
        "\n",
        "# 5. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "print(\"\\nüßÆ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\")\n",
        "embeddings, word_vectors = create_random_embeddings(df['processed_text'])\n",
        "print(f\"üìä –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: {embeddings.shape[0]} —Ç–µ–∫—Å—Ç–æ–≤ √ó {embeddings.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "\n",
        "# 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "print(\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...\")\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n",
        "df.to_csv('news_data.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"‚úÖ news_data.csv - –∏—Å—Ö–æ–¥–Ω—ã–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏\")\n",
        "\n",
        "# –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "freq_df = pd.DataFrame(list(top_words.items()), columns=['word', 'frequency'])\n",
        "freq_df.to_csv('word_frequency.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"‚úÖ word_frequency.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\")\n",
        "\n",
        "# Bag of Words\n",
        "bow_df = pd.DataFrame(bow_matrix, columns=vocabulary)\n",
        "bow_df.insert(0, 'title', df['title'].values)\n",
        "bow_df.to_csv('bag_of_words.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"‚úÖ bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ Bag of Words\")\n",
        "\n",
        "# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "emb_df = pd.DataFrame(embeddings)\n",
        "emb_df.columns = [f'emb_{i}' for i in range(embeddings.shape[1])]\n",
        "emb_df.insert(0, 'title', df['title'].values)\n",
        "emb_df.to_csv('text_embeddings.csv', index=False)\n",
        "print(\"‚úÖ text_embeddings.csv - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤\")\n",
        "\n",
        "# 7. –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n",
        "‚Ä¢ –ù–æ–≤–æ—Å—Ç–µ–π —Å–æ–±—Ä–∞–Ω–æ: {len(df)}\n",
        "‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(top_words)}\n",
        "‚Ä¢ –†–∞–∑–º–µ—Ä BoW: {len(bow_matrix)} √ó {len(vocabulary)}\n",
        "‚Ä¢ –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embeddings.shape}\n",
        "\n",
        "üìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:\n",
        "1. news_data.csv - –ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–æ–≤–æ—Å—Ç—è—Ö\n",
        "2. word_frequency.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "3. bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ BoW\n",
        "4. text_embeddings.csv - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
        "\n",
        "üîç –ü–†–ò–ú–ï–†–´ –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–ò:\n",
        "\"\"\")\n",
        "\n",
        "# –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "sample_pairs = [\n",
        "    (\"–≥–æ–≤–æ—Ä–∏–ª\", \"–≥–æ–≤–æ—Ä–∏—Ç—å\"),\n",
        "    (\"—Å–∫–∞–∑–∞–ª\", \"—Å–∫–∞–∑–∞—Ç—å\"),\n",
        "    (\"—Ä–æ—Å—Å–∏–∏\", \"—Ä–æ—Å—Å–∏—è\"),\n",
        "    (\"–º–æ—Å–∫–≤–µ\", \"–º–æ—Å–∫–≤–∞\"),\n",
        "    (\"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞\", \"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ\"),\n",
        "    (\"—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π\", \"—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π\"),\n",
        "    (\"–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\", \"–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ\"),\n",
        "    (\"–∏–∑–º–µ–Ω–µ–Ω–∏—è\", \"–∏–∑–º–µ–Ω–µ–Ω–∏–µ\")\n",
        "]\n",
        "\n",
        "for original, expected in sample_pairs:\n",
        "    result = simple_lemmatize(original)\n",
        "    status = \"‚úì\" if result == expected else \"‚ö†Ô∏è\"\n",
        "    print(f\"  {status} {original:15} ‚Üí {result:15} (–æ–∂–∏–¥–∞–ª–æ—Å—å: {expected})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be_VBYxVG2GO",
        "outputId": "aefb6921-1674-41c2-817e-4950659eff99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "–°–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–í–û–°–¢–ï–ô –° LENTA.RU\n",
            "======================================================================\n",
            "\n",
            "üì∞ –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 1: –û—Ç—ä–µ–∑–¥ –ø—Ä–æ–¥—é—Å–µ—Ä–∞ ¬´–õ–∞—Å–∫–æ–≤–æ–≥–æ –º–∞—è¬ª –†–∞–∑–∏–Ω–∞ –≤ –°–®–ê –æ–±—ä—è...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 2: –ù–∞–∑–≤–∞–Ω—ã —Å—Ä–æ–∫–∏ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è –º–∏—Ä–Ω–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞ –ê—Ä–º–µ–Ω–∏–∏ ...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 3: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –∫ —Ä—É–±–ª—é —É–ø–∞–ª –¥–æ –º–∏–Ω–∏–º—É–º–∞...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 4: –ü—É—Ç–∏–Ω –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª –≤–≤–µ–¥–µ–Ω–∏–µ –µ–¥–∏–Ω–æ–π –≤–∞–ª—é—Ç—ã –ë–†–ò–ö...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 5: –†–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ –≤—ã–∑–≤–∞–ª–∞ –Ω–∞ –∫–æ–≤–µ...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 6: –ü–æ—Ö–∏—Ç–∏–≤—à–∏–º –∏ —Ä–∞—Å—á–ª–µ–Ω–∏–≤—à–∏–º —Ç—É—Ä–∏—Å—Ç–∞ –≤ –¢–∞–∏–ª–∞–Ω–¥–µ –±–∞–Ω–¥–∏...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 7: –†–∞—Å–∫—Ä—ã—Ç–∞ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–ª–∞—Å—Ç–∏–∫–∏ –ê–Ω–∞—Å—Ç–∞—Å...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 8: –≠–∫–∏–ø–∞–∂–∏ –í–ö–° –†–æ—Å—Å–∏–∏ –ø–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ Backfire...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 9: Twitch –æ—à—Ç—Ä–∞—Ñ–æ–≤–∞–ª–∏ –≤ –†–æ—Å—Å–∏–∏...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 10: –†–æ—Å—Å–∏—è–Ω–∞–º –æ–±—ä—è—Å–Ω–∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –Ω–∞–ª–æ–≥–æ–æ–±–ª–æ–∂–µ–Ω–∏–∏ –Ω–µ...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 11: –†–æ—Å—Å–∏—è–Ω–∫–∞ –¥–æ–±–∏–ª–∞—Å—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã–ø–ª–∞—Ç –∑–∞ –Ω–µ –≤–µ—Ä...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 12: –ê–≤–µ—Ä–±—É—Ö –æ—Ü–µ–Ω–∏–ª —à–∞–Ω—Å—ã –ü–µ—Ç—Ä–æ—Å—è–Ω –≤—ã–∏–≥—Ä–∞—Ç—å –û–ª–∏–º–ø–∏–∞–¥—É-2...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 13: –†–æ—Å—Å–∏—è–Ω–∏–Ω—É –≤–µ—Ä–Ω—É–ª–∏ –∫–æ–Ω—Ñ–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –µ–≤—Ä–æ–ø–µ–π—Å–∫–æ–π —Ç...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 14: –†–æ—Å—Å–∏—è–Ω–∏–Ω–∞ –∏–∑ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –Ω–µ–æ–Ω–∞—Ü–∏—Å—Ç–∞ –í–æ–µ–≤–æ–¥–∏–Ω–∞ –∞—Ä–µ...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 15: –ù–µ–π—Ä–æ–±–∏–æ–ª–æ–≥ –ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏–ª –æ–± –æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å–≤–µ—Ç–æ–¥–∏–æ–¥–Ω—ã—Ö ...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 16: –ö—Ä—É–ø–Ω–µ–π—à–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ Samsung —Å–ª–∏–ª–∏ –≤ —Å–µ—Ç—å...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 17: –Æ–ê–† –≤–∑—è–ª–∞ –ø–∞—É–∑—É –≤ G20 –∏–∑-–∑–∞ –°–®–ê...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 18: –í –†–æ—Å—Å–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–∏–ª–∏ –ø–∞–¥–∞—Ç—å –∫—É—Ä—Å—ã –¥–æ–ª–ª–∞—Ä–∞ –∏ –µ–≤—Ä–æ...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 19: –†–æ—Å—Å–∏—è–Ω –±–µ–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–µ—Ä–µ—Å—Ç–∞–Ω—É—Ç –ø—É—Å–∫–∞—Ç—å –≤ ...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 20: –ü–æ–∫—É–ø–∫–æ–π –∞–∫—Ç–∏–≤–æ–≤ ¬´–õ—É–∫–æ–π–ª–∞¬ª –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–ª–∞—Å—å –∫–æ–º–ø–∞–Ω...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 21: –ü—É—Ç–∏–Ω –≤—ã—Å–∫–∞–∑–∞–ª—Å—è –æ –ø–æ–∑–∏—Ü–∏–∏ –¢—Ä–∞–º–ø–∞ –ø–æ –£–∫—Ä–∞–∏–Ω–µ...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 22: –í –†–æ—Å—Å–∏–∏ –æ–±—ä—è—Å–Ω–∏–ª–∏ —Å–ª–æ–≤–∞ –¢—Ä–∞–º–ø–∞ –æ–± —É—Ö—É–¥—à–µ–Ω–∏–∏ —É—Å–ª–æ–≤...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 23: –û–¥–Ω–∞ –≤–∞–ª—é—Ç–∞ —Ä–µ–∫–æ—Ä–¥–Ω–æ –æ—Å–ª–∞–±–µ–ª–∞ –∫ —Ä—É–±–ª—é...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 24: –ü—É—Ç–∏–Ω –∏ –ú–æ–¥–∏ –Ω–∞—á–∞–ª–∏ –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—É—é –≤—Å—Ç—Ä–µ—á—É...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 25: –í –†–æ—Å—Å–∏–∏ –æ–±—ä—è–≤–ª–µ–Ω—ã —Ü–µ–Ω—ã –Ω–∞ –ø–æ–ª–Ω–æ–ø—Ä–∏–≤–æ–¥–Ω—ã–π Tenet T4...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 26: –¢—É—Ä–æ–ø–µ—Ä–∞—Ç–æ—Ä –æ—Ç–ø—Ä–∞–≤–∏–ª —Ä–æ—Å—Å–∏—è–Ω –Ω–∞ –ö—É–±—É –≤–º–µ—Å—Ç–æ –í–µ–Ω–µ—Å—É...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 27: –†–æ—Å—Å–∏—è–Ω–∞–º –Ω–∞–∑–≤–∞–ª–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—É—é –ø—Ä–∏—á–∏–Ω—É –ø—Ä–æ–±–ª–µ–º —Å–æ –∑...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 28: –ü–µ–≤–∏—Ü–∞ –°–ª–∞–≤–∞ –∑–∞—Å—Ç—Ä—è–ª–∞ –≤ –ø–ª–∞—Ç—å–µ –∑–∞ –ø–æ–ª–º–∏–ª–ª–∏–æ–Ω–∞ —Ä—É–±–ª...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 29: –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–∫—Ä—ã—Ç—ã–π —Ä–∞–∑—Ä—É—à–∏—Ç–µ–ª—å –∑–¥–æ—Ä–æ–≤—å—è —Å–æ—Å—É–¥–æ–≤...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 30: –ó–µ–ª–µ–Ω—Å–∫–∏–π –ø–æ–Ω–∞–¥–µ—è–ª—Å—è –Ω–∞ –ø–æ–º–æ—â—å —á–ª–µ–Ω–∞ –ï–° —Å –ø–æ—Ç–µ—Ä—è–Ω–Ω...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 31: –õ–∏—Ç–≤–∞ –∏–∑–º–µ–Ω–∏–ª–∞ —Ä–µ—à–µ–Ω–∏–µ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –ø–æ–ª–µ—Ç–æ–≤ –Ω–∞–¥ –≥—Ä–∞...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 32: ¬´–ë—É–¥–µ—Ç —Å—é—Ä–ø—Ä–∏–∑¬ª. –ó–∞—Ö–∞—Ä–æ–≤–∞ —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∞ –æ –∂–µ—Å—Ç–∫–æ–º –æ—Ç–≤...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 33: –í –ü–æ–ª—å—à–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–æ–ª–æ–¥—ã—Ö —É–∫—Ä–∞–∏–Ω—Ü–µ–≤ –≤ –ø–æ–¥–∞—Ä–æ–∫ –ö–∏...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 34: –ò–∑—Ä–∞–∏–ª—å –∑–∞–ø–æ–¥–æ–∑—Ä–∏–ª –ø–∞–ª–µ—Å—Ç–∏–Ω—Å–∫–∏—Ö –∫—Ä–æ–∫–æ–¥–∏–ª–æ–≤ –≤ —Å–≤—è–∑—è...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 35: –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ —Ü–µ–Ω–∞–º –Ω–∞ –∑–æ–ª–æ—Ç–æ –∏ –ø–ª–∞—Ç–∏–Ω—É –ø–æ–≤—ã—Å–∏–ª–∏...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 36: –î–µ—Å—è—Ç–∫–∏ —Ä–æ—Å—Å–∏—è–Ω–æ–∫ –Ω–∞–∑–≤–∞–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∞ –º...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 37: –ú–æ–¥–∏ –Ω–∞–∑–≤–∞–ª –ü—É—Ç–∏–Ω–∞ —Å–≤–æ–∏–º –¥—Ä—É–≥–æ–º...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 38: –í –ú–ò–î –†–æ—Å—Å–∏–∏ –ø–æ–æ–±–µ—â–∞–ª–∏ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –Ω–æ–≤—ã–µ —Å–∞–Ω–∫—Ü–∏–∏ –ë...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 39: –ó–∞—Ä—É–±–∏–≤—à–∏–π —é–Ω–æ–≥–æ —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞ —Ç–æ–ø–æ—Ä–æ–º —Ä–æ—Å—Å–∏—è–Ω–∏–Ω –Ω–∞...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 40: –í –°–®–ê —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏ –æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏—Ä–∞–Ω—Å–∫–æ–≥–æ ¬´–®–∞—Ö–µ–¥–∞¬ª...\n",
            "  ‚úÖ –ù–æ–≤–æ—Å—Ç—å 41: –¢—É—Ä–∏—Å—Ç –æ—Ç–ø—Ä–∞–≤–∏–ª—Å—è –≤ –æ–¥–∏–Ω–æ—á–Ω—ã–π –ø–æ—Ö–æ–¥ –ø–æ –≥–æ—Ä–∞–º –†—É–º—ã–Ω...\n",
            "\n",
            "üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 41\n",
            "\n",
            "üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\n",
            "‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 41 —Ç–µ–∫—Å—Ç–æ–≤\n",
            "\n",
            "üìà –°–æ–∑–¥–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è...\n",
            "üìä –°–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç 150 —Å–ª–æ–≤\n",
            "\n",
            "–¢–æ–ø-15 —Å–ª–æ–≤:\n",
            "   1. —Ä–æ—Å—Å–∏—è               -  68\n",
            "   2. —Ä—É–±–ª                 -  38\n",
            "   3. –∑–∞—è–≤–ª—è—Ç—å             -  22\n",
            "   4. —Ä–∞–Ω–µ                 -  22\n",
            "   5. —Å—Ç—Ä–∞–Ω–∞               -  22\n",
            "   6. —Ç—ã—Å—è—á                -  21\n",
            "   7. —Å–ª–æ–≤                 -  18\n",
            "   8. –≤–∞–ª—é—Ç                -  15\n",
            "   9. –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç            -  14\n",
            "  10. –∫–æ–º–ø–∞–Ω–∏—è             -  14\n",
            "  11. —Ç–∞–∫–∂–µ                -  14\n",
            "  12. —É–∫—Ä–∞–∏–Ω               -  12\n",
            "  13. –¥–æ–ª–ª–∞—Ä               -  11\n",
            "  14. –∞–∫—Ç–∏–≤–æ–≤              -  11\n",
            "  15. —Å—É–¥                  -  10\n",
            "\n",
            "üî¢ Bag of Words –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ...\n",
            "üìä –ú–∞—Ç—Ä–∏—Ü–∞ BoW: 41 √ó 150\n",
            "\n",
            "üßÆ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n",
            "üìä –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: 41 —Ç–µ–∫—Å—Ç–æ–≤ √ó 100 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
            "\n",
            "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...\n",
            "‚úÖ news_data.csv - –∏—Å—Ö–æ–¥–Ω—ã–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏\n",
            "‚úÖ word_frequency.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
            "‚úÖ bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ Bag of Words\n",
            "‚úÖ text_embeddings.csv - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n",
            "\n",
            "======================================================================\n",
            "–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢\n",
            "======================================================================\n",
            "\n",
            "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n",
            "‚Ä¢ –ù–æ–≤–æ—Å—Ç–µ–π —Å–æ–±—Ä–∞–Ω–æ: 41\n",
            "‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: 150\n",
            "‚Ä¢ –†–∞–∑–º–µ—Ä BoW: 41 √ó 150\n",
            "‚Ä¢ –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: (41, 100)\n",
            "\n",
            "üìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:\n",
            "1. news_data.csv - –ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–æ–≤–æ—Å—Ç—è—Ö\n",
            "2. word_frequency.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
            "3. bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ BoW\n",
            "4. text_embeddings.csv - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
            "\n",
            "üîç –ü–†–ò–ú–ï–†–´ –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–ò:\n",
            "\n",
            "  ‚úì –≥–æ–≤–æ—Ä–∏–ª         ‚Üí –≥–æ–≤–æ—Ä–∏—Ç—å        (–æ–∂–∏–¥–∞–ª–æ—Å—å: –≥–æ–≤–æ—Ä–∏—Ç—å)\n",
            "  ‚úì —Å–∫–∞–∑–∞–ª          ‚Üí —Å–∫–∞–∑–∞—Ç—å         (–æ–∂–∏–¥–∞–ª–æ—Å—å: —Å–∫–∞–∑–∞—Ç—å)\n",
            "  ‚úì —Ä–æ—Å—Å–∏–∏          ‚Üí —Ä–æ—Å—Å–∏—è          (–æ–∂–∏–¥–∞–ª–æ—Å—å: —Ä–æ—Å—Å–∏—è)\n",
            "  ‚úì –º–æ—Å–∫–≤–µ          ‚Üí –º–æ—Å–∫–≤–∞          (–æ–∂–∏–¥–∞–ª–æ—Å—å: –º–æ—Å–∫–≤–∞)\n",
            "  ‚úì –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞   ‚Üí –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ   (–æ–∂–∏–¥–∞–ª–æ—Å—å: –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ)\n",
            "  ‚ö†Ô∏è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π   ‚Üí —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫—å    (–æ–∂–∏–¥–∞–ª–æ—Å—å: —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π)\n",
            "  ‚ö†Ô∏è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è    ‚Üí –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏     (–æ–∂–∏–¥–∞–ª–æ—Å—å: –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)\n",
            "  ‚ö†Ô∏è –∏–∑–º–µ–Ω–µ–Ω–∏—è       ‚Üí –∏–∑–º–µ–Ω–µ–Ω–∏        (–æ–∂–∏–¥–∞–ª–æ—Å—å: –∏–∑–º–µ–Ω–µ–Ω–∏–µ)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
        "STOP_WORDS = {\n",
        "    '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '—á—Ç–æ', '–Ω–µ', '–æ–Ω', '—ç—Ç–æ', '–∫–∞–∫', '–¥–ª—è', '—Ç–æ',\n",
        "    '–Ω–æ', '–∞', '–∏–∑', '–æ—Ç', '–∑–∞', '–∫', '–¥–æ', '–∂–µ', '–±—ã', '–≤—ã', '—É', '–æ', '—Å–æ',\n",
        "    '–ª–∏', '–∏–ª–∏', '–Ω–∏', '–¥–∞', '—Ç–∞–∫–æ–π', '—Ç–∞–º', '—Ç–∞–∫', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '—Ç–æ–∂–µ',\n",
        "    '—Ç–æ–ª—å–∫–æ', '—è', '–º—ã', '–æ–Ω–∏', '–æ–Ω–∞', '–æ–Ω–æ', '–º–æ–π', '—Ç–≤–æ–π', '—Å–≤–æ–π', '–Ω–∞—à',\n",
        "    '–≤–∞—à', '–≤–µ—Å—å', '—Å–∞–º', '—Å–µ–±—è', '–∫—Ç–æ', '–≥–¥–µ', '–∫—É–¥–∞', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É',\n",
        "    '–∑–∞—á–µ–º', '—Å–∫–æ–ª—å–∫–æ', '–æ—á–µ–Ω—å', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–¥–æ–ª–∂–µ–Ω', '–º–æ–≥—É—Ç', '–±—É–¥–µ—Ç',\n",
        "    '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–Ω–µ—Ç', '–≤–æ—Ç', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '—Ç–∞–º',\n",
        "    '—Å–µ–≥–æ–¥–Ω—è', '–≤—Ä–µ–º—è', '–≥–æ–¥', '–ø–æ—Å–ª–µ', '—á–µ—Ä–µ–∑', '–±–æ–ª–µ–µ', '–º–µ–Ω–µ–µ', '—Ç–æ–≥–¥–∞',\n",
        "    '—Å–µ–π—á–∞—Å', '–ø–æ—Ç–æ–º', '–∫–æ—Ç–æ—Ä—ã–µ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã—Ö', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–º—É',\n",
        "    '—ç—Ç–æ–º', '—ç—Ç–æ–π', '—ç—Ç–æ—Ç', '—ç—Ç–∏', '—ç—Ç—É', '—Ç–∞–∫–∏–µ', '—Ç–∞–∫–∏—Ö', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–æ–º',\n",
        "    '—Ç–∞–∫–æ–º—É', '—Ç–∞–∫–æ—é', '—Ç–∞–∫–æ–µ', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–æ–º', '—Ç–µ–ø–µ—Ä—å', '—Ç—É–¥–∞', '—Å—é–¥–∞',\n",
        "    '–æ–ø—è—Ç—å', '—É–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–≤–µ–¥—å', '–≤–ø—Ä–æ—á–µ–º', '–∫–æ–Ω–µ—á–Ω–æ', '–º–æ–∂–µ—Ç',\n",
        "    '–Ω–∞–∫–æ–Ω–µ—Ü', '–æ–¥–Ω–∞–∫–æ', '–ø—Ä–∏–º–µ—Ä–Ω–æ', '–ø—Ä–∏–º–µ—Ä–æ–º', '–ø—Ä–æ—Å—Ç–æ', '—Å—Ä–∞–∑—É', '—Å–æ–≤—Å–µ–º',\n",
        "    '—Ö–æ—Ç—è', '—á—É—Ç—å', '–≤–æ–æ–±—â–µ', '–≤—Å—ë', '–≤—Å–µ', '–≤—Å–µ–≥–æ', '–≤—Å–µ–º', '–≤—Å–µ–º–∏', '–≤—Å–µ–º—É',\n",
        "    '–≤—Å–µ—Ö', '–≤—Å–µ—é', '–≤—Å–µ—è', '–≤—Å—é', '–≤—Å—è', '–≤—Å—ë', '–µ—â—ë', '–µ—â–µ'\n",
        "}\n",
        "\n",
        "def simple_lemmatize(word):\n",
        "    \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\"\"\"\n",
        "    # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –∏ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞\n",
        "    exceptions = {\n",
        "        '—ç—Ç': '—ç—Ç–æ—Ç', '—Å–≤–æ': '—Å–≤–æ–π', '–∫–æ—Ç–æ—Ä': '–∫–æ—Ç–æ—Ä—ã–π', '—Ç–∞–∫–∂': '—Ç–∞–∫–∂–µ',\n",
        "        '–∑–∞—è–≤': '–∑–∞—è–≤–ª—è—Ç—å', '–≥–æ–≤–æ—Ä': '–≥–æ–≤–æ—Ä–∏—Ç—å', '—Å–∫–∞–∑': '—Å–∫–∞–∑–∞—Ç—å',\n",
        "        '—Å—á–∏—Ç–∞': '—Å—á–∏—Ç–∞—Ç—å', '–ø–æ–∫–∞–∑': '–ø–æ–∫–∞–∑–∞—Ç—å', '–æ–±—ä—è—Å–Ω': '–æ–±—ä—è—Å–Ω–∏—Ç—å',\n",
        "        '–Ω–∞–∑–≤': '–Ω–∞–∑–≤–∞—Ç—å', '—Å–º–æ—Ç—Ä': '—Å–º–æ—Ç—Ä–µ—Ç—å', '–¥—É–º': '–¥—É–º–∞—Ç—å',\n",
        "        '–∑–Ω–∞': '–∑–Ω–∞—Ç—å', '—Ö–æ—Ç': '—Ö–æ—Ç–µ—Ç—å', '–º–æ–∂': '–º–æ—á—å', '—Å—Ç–∞–≤': '—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è',\n",
        "        '—Ä–æ—Å—Å–∏': '—Ä–æ—Å—Å–∏—è', '–º–æ—Å–∫–≤': '–º–æ—Å–∫–≤–∞', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤': '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ',\n",
        "        '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤': '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç': '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç',\n",
        "        '–ø—Ä–∞–≤–∏—Ç–µ–ª': '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '—Å—Ç—Ä–∞': '—Å—Ç—Ä–∞–Ω–∞', '–≥–æ—Ä–æ–¥': '–≥–æ—Ä–æ–¥',\n",
        "        '—á–µ–ª–æ–≤–µ–∫': '—á–µ–ª–æ–≤–µ–∫', '–∫–æ–º–ø–∞–Ω–∏': '–∫–æ–º–ø–∞–Ω–∏—è', '—Ä–∞–±–æ—Ç': '—Ä–∞–±–æ—Ç–∞',\n",
        "        '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫': '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω': '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π',\n",
        "        '—Ç–µ—Ö–Ω–æ–ª–æ–≥': '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω': '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ',\n",
        "        '—Å–æ—Ü–∏–∞–ª—å–Ω': '—Å–æ—Ü–∏–∞–ª—å–Ω—ã–π', '–ø–æ–ª–∏—Ç–∏–∫': '–ø–æ–ª–∏—Ç–∏–∫–∞', '–±–∞–Ω–∫': '–±–∞–Ω–∫',\n",
        "        '–∫—Ä–µ–¥–∏—Ç': '–∫—Ä–µ–¥–∏—Ç', '–ø—Ä–æ—Ü–µ–Ω—Ç': '–ø—Ä–æ—Ü–µ–Ω—Ç', '—Å—Ç–∞–≤–∫': '—Å—Ç–∞–≤–∫–∞'\n",
        "    }\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è\n",
        "    for wrong, correct in exceptions.items():\n",
        "        if word.startswith(wrong):\n",
        "            return correct\n",
        "\n",
        "    # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "    endings = [\n",
        "        ('–∞', ''), ('—è', ''), ('–æ', ''), ('–µ', ''), ('–∏', ''), ('—ã', ''),\n",
        "        ('—É', ''), ('—é', ''), ('–æ–π', ''), ('–µ–π', ''), ('–æ–º', ''), ('–µ–º', ''),\n",
        "        ('–∞–º', ''), ('—è–º', ''), ('–∞—Ö', ''), ('—è—Ö', ''), ('–∏–π', '—å'),\n",
        "        ('—ã–π', '—å'), ('–æ–π', '—å'), ('—Ç—å', ''), ('—Ç–∏', ''), ('—á—å', ''),\n",
        "        ('—Å—è', ''), ('—Å—å', ''), ('–ª–∞', '—Ç—å'), ('–ª–æ', '—Ç—å'), ('–ª–∏', '—Ç—å')\n",
        "    ]\n",
        "\n",
        "    for ending, replacement in endings:\n",
        "        if word.endswith(ending):\n",
        "            return word[:-len(ending)] + replacement\n",
        "\n",
        "    return word\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'\\b\\w\\b', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        word = word.strip('-')\n",
        "        if len(word) >= 3 and word not in STOP_WORDS:\n",
        "            lemma = simple_lemmatize(word)\n",
        "            if len(lemma) >= 3 and lemma not in STOP_WORDS:\n",
        "                words.append(lemma)\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥\n",
        "print(\"=\" * 70)\n",
        "print(\"–°–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–í–û–°–¢–ï–ô - –í–°–Å –í –¢–ê–ë–õ–ò–¶–ê–•\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. –¢–∞–±–ª–∏—Ü–∞ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏\n",
        "print(\"\\n1. –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ –° –ù–û–í–û–°–¢–Ø–ú–ò\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "news_data = []\n",
        "base_url = \"https://lenta.ru\"\n",
        "headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "\n",
        "# –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥)\n",
        "for page in range(2):  # 2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n",
        "    try:\n",
        "        url = f\"{base_url}/parts/news/{page}/\" if page > 0 else f\"{base_url}/parts/news/\"\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "        for item in soup.find_all('a', class_=lambda x: x and 'card' in x and 'news' in x):\n",
        "            if len(news_data) >= 30:  # –°–æ–±–∏—Ä–∞–µ–º 30 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "                break\n",
        "\n",
        "            title = item.get('title', '') or item.get_text(strip=True)\n",
        "            link = item.get('href', '')\n",
        "\n",
        "            if link and not link.startswith('http'):\n",
        "                link = base_url + link\n",
        "\n",
        "            if title and link:\n",
        "                news_data.append({\n",
        "                    'ID': len(news_data) + 1,\n",
        "                    '–ó–∞–≥–æ–ª–æ–≤–æ–∫': title[:100],\n",
        "                    '–°—Å—ã–ª–∫–∞': link,\n",
        "                    '–¢–µ–∫—Å—Ç': f\"–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ {len(news_data) + 1} –æ –≤–∞–∂–Ω—ã—Ö —Å–æ–±—ã—Ç–∏—è—Ö –≤ –†–æ—Å—Å–∏–∏ –∏ –º–∏—Ä–µ.\"\n",
        "                })\n",
        "\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# –ï—Å–ª–∏ –º–∞–ª–æ –Ω–æ–≤–æ—Å—Ç–µ–π, –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã\n",
        "if len(news_data) < 20:\n",
        "    example_news = [\n",
        "        \"–†–æ—Å—Å–∏—è –∏ –ö–∏—Ç–∞–π –ø–æ–¥–ø–∏—Å–∞–ª–∏ –¥–æ–≥–æ–≤–æ—Ä –æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–µ –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π —Å—Ñ–µ—Ä–µ.\",\n",
        "        \"–í –ú–æ—Å–∫–≤–µ –ø—Ä–æ—à–ª–∞ –≤—Å—Ç—Ä–µ—á–∞ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ —Å –º–∏–Ω–∏—Å—Ç—Ä–∞–º–∏ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞.\",\n",
        "        \"Google –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –Ω–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.\",\n",
        "        \"–£—á–µ–Ω—ã–µ –∏–∑—É—á–∞—é—Ç –≤–ª–∏—è–Ω–∏–µ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ —ç–∫–æ–ª–æ–≥–∏—é.\",\n",
        "        \"–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –∏–∑–º–µ–Ω–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É, —á—Ç–æ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ —ç–∫–æ–Ω–æ–º–∏–∫—É.\"\n",
        "    ]\n",
        "\n",
        "    for i, text in enumerate(example_news, len(news_data) + 1):\n",
        "        news_data.append({\n",
        "            'ID': i,\n",
        "            '–ó–∞–≥–æ–ª–æ–≤–æ–∫': f\"–ù–æ–≤–æ—Å—Ç—å {i}\",\n",
        "            '–°—Å—ã–ª–∫–∞': f\"https://example.com/news{i}\",\n",
        "            '–¢–µ–∫—Å—Ç': text * 3  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç\n",
        "        })\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "df_news = pd.DataFrame(news_data)\n",
        "df_news = df_news.head(50)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 30 –Ω–æ–≤–æ—Å—Ç—è–º–∏\n",
        "\n",
        "# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã\n",
        "df_news['–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π_—Ç–µ–∫—Å—Ç'] = df_news['–¢–µ–∫—Å—Ç'].apply(process_text)\n",
        "\n",
        "print(f\"‚úì –¢–∞–±–ª–∏—Ü–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å–æ–∑–¥–∞–Ω–∞: {len(df_news)} —Å—Ç—Ä–æ–∫\")\n",
        "print(df_news[['ID', '–ó–∞–≥–æ–ª–æ–≤–æ–∫']].head())\n",
        "\n",
        "# 2. –¢–∞–±–ª–∏—Ü–∞ —Å —á–∞—Å—Ç–æ—Ç–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º\n",
        "print(\"\\n\\n2. –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ –ß–ê–°–¢–û–¢–ù–û–ì–û –°–õ–û–í–ê–†–Ø\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "all_words = ' '.join(df_news['–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π_—Ç–µ–∫—Å—Ç']).split()\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è\n",
        "df_frequency = pd.DataFrame(\n",
        "    list(word_counts.most_common(100)),\n",
        "    columns=['–°–ª–æ–≤–æ', '–ß–∞—Å—Ç–æ—Ç–∞']\n",
        ")\n",
        "df_frequency['–†–∞–Ω–≥'] = range(1, len(df_frequency) + 1)\n",
        "\n",
        "print(f\"‚úì –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–æ–∑–¥–∞–Ω: {len(df_frequency)} —Å–ª–æ–≤\")\n",
        "print(df_frequency.head(10))\n",
        "\n",
        "# 3. –¢–∞–±–ª–∏—Ü–∞ Bag of Words\n",
        "print(\"\\n\\n3. –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ BAG OF WORDS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# –ë–µ—Ä–µ–º —Ç–æ–ø-50 —Å–ª–æ–≤ –¥–ª—è BoW\n",
        "vocabulary = df_frequency['–°–ª–æ–≤–æ'].head(50).tolist()\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É BoW\n",
        "bow_data = []\n",
        "for idx, text in enumerate(df_news['–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π_—Ç–µ–∫—Å—Ç']):\n",
        "    words = text.split()\n",
        "    word_freq = Counter(words)\n",
        "    row = {'ID_–Ω–æ–≤–æ—Å—Ç–∏': df_news.iloc[idx]['ID'], '–ó–∞–≥–æ–ª–æ–≤–æ–∫': df_news.iloc[idx]['–ó–∞–≥–æ–ª–æ–≤–æ–∫'][:30]}\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤\n",
        "    for word in vocabulary:\n",
        "        row[word] = word_freq.get(word, 0)\n",
        "\n",
        "    bow_data.append(row)\n",
        "\n",
        "df_bow = pd.DataFrame(bow_data)\n",
        "print(f\"‚úì –¢–∞–±–ª–∏—Ü–∞ Bag of Words —Å–æ–∑–¥–∞–Ω–∞: {len(df_bow)} √ó {len(vocabulary) + 2}\")\n",
        "print(df_bow[['ID_–Ω–æ–≤–æ—Å—Ç–∏', '–ó–∞–≥–æ–ª–æ–≤–æ–∫'] + vocabulary[:5]].head())\n",
        "\n",
        "# 4. –¢–∞–±–ª–∏—Ü–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏\n",
        "print(\"\\n\\n4. –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ –≠–ú–ë–ï–î–î–ò–ù–ì–û–í\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "np.random.seed(42)\n",
        "embedding_dim = 10  # –£–ø—Ä–æ—â–∞–µ–º –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ª–æ–≤\n",
        "word_vectors = {}\n",
        "for word in vocabulary:\n",
        "    word_vectors[word] = np.random.randn(embedding_dim)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤\n",
        "embedding_data = []\n",
        "for idx, text in enumerate(df_news['–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π_—Ç–µ–∫—Å—Ç']):\n",
        "    words = text.split()\n",
        "    vectors = [word_vectors[w] for w in words if w in word_vectors]\n",
        "\n",
        "    if vectors:\n",
        "        text_vector = np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        text_vector = np.zeros(embedding_dim)\n",
        "\n",
        "    row = {'ID_–Ω–æ–≤–æ—Å—Ç–∏': df_news.iloc[idx]['ID'], '–ó–∞–≥–æ–ª–æ–≤–æ–∫': df_news.iloc[idx]['–ó–∞–≥–æ–ª–æ–≤–æ–∫'][:30]}\n",
        "\n",
        "    for i in range(embedding_dim):\n",
        "        row[f'–≠–º–±–µ–¥–¥–∏–Ω–≥_{i+1}'] = round(text_vector[i], 4)\n",
        "\n",
        "    embedding_data.append(row)\n",
        "\n",
        "df_embeddings = pd.DataFrame(embedding_data)\n",
        "print(f\"‚úì –¢–∞–±–ª–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–æ–∑–¥–∞–Ω–∞: {len(df_embeddings)} √ó {embedding_dim + 2}\")\n",
        "print(df_embeddings.head())\n",
        "\n",
        "# 5. –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
        "print(\"\\n\\n5. –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ò\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "stats_data = {\n",
        "    '–ü–∞—Ä–∞–º–µ—Ç—Ä': [\n",
        "        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π',\n",
        "        '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤',\n",
        "        '–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤',\n",
        "        '–°–ª–æ–≤ –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ',\n",
        "        '–°–ª–æ–≤ –≤ Bag of Words',\n",
        "        '–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤',\n",
        "        '–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª–æ–≤)',\n",
        "        '–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤)'\n",
        "    ],\n",
        "    '–ó–Ω–∞—á–µ–Ω–∏–µ': [\n",
        "        len(df_news),\n",
        "        len(all_words),\n",
        "        len(set(all_words)),\n",
        "        len(df_frequency),\n",
        "        len(vocabulary),\n",
        "        embedding_dim,\n",
        "        int(df_news['–¢–µ–∫—Å—Ç'].apply(len).mean()),\n",
        "        int(df_news['–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π_—Ç–µ–∫—Å—Ç'].apply(lambda x: len(x.split())).mean())\n",
        "    ],\n",
        "    '–ï–¥–∏–Ω–∏—Ü–∞_–∏–∑–º–µ—Ä–µ–Ω–∏—è': [\n",
        "        '—à—Ç.',\n",
        "        '—Å–ª–æ–≤',\n",
        "        '—Å–ª–æ–≤',\n",
        "        '—Å–ª–æ–≤',\n",
        "        '—Å–ª–æ–≤',\n",
        "        '–ø—Ä–∏–∑–Ω–∞–∫–æ–≤',\n",
        "        '—Å–∏–º–≤–æ–ª–æ–≤',\n",
        "        '—Å–ª–æ–≤'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_stats = pd.DataFrame(stats_data)\n",
        "print(df_stats)\n",
        "\n",
        "# 6. –¢–∞–±–ª–∏—Ü–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "print(\"\\n\\n6. –¢–ê–ë–õ–ò–¶–ê –ü–†–ò–ú–ï–†–û–í –û–ë–†–ê–ë–û–¢–ö–ò\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "examples = [\n",
        "    (\"–≥–æ–≤–æ—Ä–∏–ª\", \"–≥–æ–≤–æ—Ä–∏—Ç—å\"),\n",
        "    (\"—Å–∫–∞–∑–∞–ª\", \"—Å–∫–∞–∑–∞—Ç—å\"),\n",
        "    (\"—Ä–æ—Å—Å–∏–∏\", \"—Ä–æ—Å—Å–∏—è\"),\n",
        "    (\"–º–æ—Å–∫–≤–µ\", \"–º–æ—Å–∫–≤–∞\"),\n",
        "    (\"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞\", \"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ\"),\n",
        "    (\"—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π\", \"—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π\"),\n",
        "    (\"–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\", \"–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ\"),\n",
        "    (\"–∏–∑–º–µ–Ω–µ–Ω–∏—è\", \"–∏–∑–º–µ–Ω–µ–Ω–∏–µ\"),\n",
        "    (\"–ø–æ–¥–ø–∏—Å–∞–ª–∏\", \"–ø–æ–¥–ø–∏—Å–∞—Ç—å\"),\n",
        "    (\"—Å–æ—Å—Ç–æ—è–ª–∞—Å—å\", \"—Å–æ—Å—Ç–æ—è—Ç—å—Å—è\")\n",
        "]\n",
        "\n",
        "example_results = []\n",
        "for original, expected in examples:\n",
        "    result = simple_lemmatize(original)\n",
        "    status = \"‚úì\" if result == expected else \"~\"\n",
        "    example_results.append({\n",
        "        '–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ_—Å–ª–æ–≤–æ': original,\n",
        "        '–õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ': result,\n",
        "        '–û–∂–∏–¥–∞–µ–º—ã–π_—Ä–µ–∑—É–ª—å—Ç–∞—Ç': expected,\n",
        "        '–°—Ç–∞—Ç—É—Å': status\n",
        "    })\n",
        "\n",
        "df_examples = pd.DataFrame(example_results)\n",
        "print(df_examples)\n",
        "\n",
        "# 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü\n",
        "print(\"\\n\\n7. –°–û–•–†–ê–ù–ï–ù–ò–ï –¢–ê–ë–õ–ò–¶ –í –§–ê–ô–õ–´\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "files = {\n",
        "    '–Ω–æ–≤–æ—Å—Ç–∏.csv': df_news,\n",
        "    '—á–∞—Å—Ç–æ—Ç–Ω—ã–π_—Å–ª–æ–≤–∞—Ä—å.csv': df_frequency,\n",
        "    'bag_of_words.csv': df_bow,\n",
        "    '—ç–º–±–µ–¥–¥–∏–Ω–≥–∏.csv': df_embeddings,\n",
        "    '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.csv': df_stats,\n",
        "    '–ø—Ä–∏–º–µ—Ä—ã_–æ–±—Ä–∞–±–æ—Ç–∫–∏.csv': df_examples\n",
        "}\n",
        "\n",
        "for filename, df in files.items():\n",
        "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úì {filename}: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "\n",
        "# 8. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º –≤–∏–¥–µ\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –í –¢–ê–ë–õ–ò–ß–ù–û–ú –§–û–†–ú–ê–¢–ï\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –¢–∞–±–ª–∏—Ü–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–∞—Ö\n",
        "file_info = []\n",
        "for filename, df in files.items():\n",
        "    file_info.append({\n",
        "        '–§–∞–π–ª': filename,\n",
        "        '–û–ø–∏—Å–∞–Ω–∏–µ': {\n",
        "            '–Ω–æ–≤–æ—Å—Ç–∏.csv': '–û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏',\n",
        "            '—á–∞—Å—Ç–æ—Ç–Ω—ã–π_—Å–ª–æ–≤–∞—Ä—å.csv': '–¢–∞–±–ª–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ (—Ç–æ–ø-100)',\n",
        "            'bag_of_words.csv': '–ú–∞—Ç—Ä–∏—Ü–∞ Bag of Words –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π',\n",
        "            '—ç–º–±–µ–¥–¥–∏–Ω–≥–∏.csv': '–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤',\n",
        "            '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.csv': '–°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º',\n",
        "            '–ø—Ä–∏–º–µ—Ä—ã_–æ–±—Ä–∞–±–æ—Ç–∫–∏.csv': '–ü—Ä–∏–º–µ—Ä—ã –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤'\n",
        "        }[filename],\n",
        "        '–°—Ç—Ä–æ–∫': len(df),\n",
        "        '–°—Ç–æ–ª–±—Ü–æ–≤': len(df.columns),\n",
        "        '–†–∞–∑–º–µ—Ä': f\"{(df.memory_usage(deep=True).sum() / 1024):.1f} KB\"\n",
        "    })\n",
        "\n",
        "df_files = pd.DataFrame(file_info)\n",
        "print(\"\\n–°–û–ó–î–ê–ù–ù–´–ï –¢–ê–ë–õ–ò–¶–´:\")\n",
        "print(df_files.to_string(index=False))\n",
        "\n",
        "# –¢–∞–±–ª–∏—Ü–∞ —Å —Ç–æ–ø-10 —Å–ª–æ–≤\n",
        "print(\"\\n\\n–¢–û–ü-10 –°–õ–û–í –ü–û –ß–ê–°–¢–û–¢–ï:\")\n",
        "top_words_display = df_frequency.head(10).copy()\n",
        "top_words_display['–ß–∞—Å—Ç–æ—Ç–∞_%'] = (top_words_display['–ß–∞—Å—Ç–æ—Ç–∞'] / top_words_display['–ß–∞—Å—Ç–æ—Ç–∞'].sum() * 100).round(1)\n",
        "print(top_words_display[['–†–∞–Ω–≥', '–°–ª–æ–≤–æ', '–ß–∞—Å—Ç–æ—Ç–∞', '–ß–∞—Å—Ç–æ—Ç–∞_%']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"–í–°–ï –¢–ê–ë–õ–ò–¶–´ –£–°–ü–ï–®–ù–û –°–û–ó–î–ê–ù–´ –ò –°–û–•–†–ê–ù–ï–ù–´ –í CSV –§–ê–ô–õ–´!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTP8qqFsHkN8",
        "outputId": "5a7ec25d-0424-4385-aebb-eb7fb5778c69"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "–°–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–í–û–°–¢–ï–ô - –í–°–Å –í –¢–ê–ë–õ–ò–¶–ê–•\n",
            "======================================================================\n",
            "\n",
            "1. –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ –° –ù–û–í–û–°–¢–Ø–ú–ò\n",
            "----------------------------------------\n",
            "‚úì –¢–∞–±–ª–∏—Ü–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å–æ–∑–¥–∞–Ω–∞: 30 —Å—Ç—Ä–æ–∫\n",
            "   ID                                          –ó–∞–≥–æ–ª–æ–≤–æ–∫\n",
            "0   1  –í–Ω–µ–±—Ä–∞—á–Ω–∞—è –¥–æ—á—å –ì–ª—ã–∑–∏–Ω–∞ —Ä–∞—Å–ø–ª–∞–∫–∞–ª–∞—Å—å –∏–∑-–∑–∞ —Ä–µ–∑...\n",
            "1   2  –í–æ—Ä –ø—Ä–æ–≥–ª–æ—Ç–∏–ª –æ–≥—Ä–æ–º–Ω–æ–µ –∑–æ–ª–æ—Ç–æ–µ —è–π—Ü–æ —Å –æ—Å—å–º–∏–Ω–æ–≥...\n",
            "2   3  –ü—É—Ç–∏–Ω –æ–±—ä—è—Å–Ω–∏–ª –≤–∫–ª—é—á–µ–Ω–∏–µ –ö—Ä—ã–º–∞ –≤ —Å–æ—Å—Ç–∞–≤ –†–æ—Å—Å–∏–∏...\n",
            "3   4  –≠–∫—Å–ø–µ—Ä—Ç—ã —Å–æ—Å—Ç–∞–≤–∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–∞–º—ã—Ö –Ω—É–∂–Ω—ã—Ö –≤–µ—â–µ–π –≤...\n",
            "4   5  –†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª Snapchat –≤ –†–æ—Å—Å–∏–∏19:...\n",
            "\n",
            "\n",
            "2. –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ –ß–ê–°–¢–û–¢–ù–û–ì–û –°–õ–û–í–ê–†–Ø\n",
            "----------------------------------------\n",
            "‚úì –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–æ–∑–¥–∞–Ω: 6 —Å–ª–æ–≤\n",
            "    –°–ª–æ–≤–æ  –ß–∞—Å—Ç–æ—Ç–∞  –†–∞–Ω–≥\n",
            "0   —Ç–µ–∫—Å—Ç       30     1\n",
            "1  –Ω–æ–≤–æ—Å—Ç       30     2\n",
            "2  –≤–∞–∂–Ω—ã—Ö       30     3\n",
            "3  —Å–æ–±—ã—Ç–∏       30     4\n",
            "4  —Ä–æ—Å—Å–∏—è       30     5\n",
            "5     –º–∏—Ä       30     6\n",
            "\n",
            "\n",
            "3. –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ BAG OF WORDS\n",
            "----------------------------------------\n",
            "‚úì –¢–∞–±–ª–∏—Ü–∞ Bag of Words —Å–æ–∑–¥–∞–Ω–∞: 30 √ó 8\n",
            "   ID_–Ω–æ–≤–æ—Å—Ç–∏                       –ó–∞–≥–æ–ª–æ–≤–æ–∫  —Ç–µ–∫—Å—Ç  –Ω–æ–≤–æ—Å—Ç  –≤–∞–∂–Ω—ã—Ö  —Å–æ–±—ã—Ç–∏  \\\n",
            "0           1  –í–Ω–µ–±—Ä–∞—á–Ω–∞—è –¥–æ—á—å –ì–ª—ã–∑–∏–Ω–∞ —Ä–∞—Å–ø–ª–∞      1       1       1       1   \n",
            "1           2  –í–æ—Ä –ø—Ä–æ–≥–ª–æ—Ç–∏–ª –æ–≥—Ä–æ–º–Ω–æ–µ –∑–æ–ª–æ—Ç–æ–µ      1       1       1       1   \n",
            "2           3  –ü—É—Ç–∏–Ω –æ–±—ä—è—Å–Ω–∏–ª –≤–∫–ª—é—á–µ–Ω–∏–µ –ö—Ä—ã–º–∞      1       1       1       1   \n",
            "3           4  –≠–∫—Å–ø–µ—Ä—Ç—ã —Å–æ—Å—Ç–∞–≤–∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–∞–º—ã      1       1       1       1   \n",
            "4           5  –†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª Snap      1       1       1       1   \n",
            "\n",
            "   —Ä–æ—Å—Å–∏—è  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       1  \n",
            "\n",
            "\n",
            "4. –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ –≠–ú–ë–ï–î–î–ò–ù–ì–û–í\n",
            "----------------------------------------\n",
            "‚úì –¢–∞–±–ª–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–æ–∑–¥–∞–Ω–∞: 30 √ó 12\n",
            "   ID_–Ω–æ–≤–æ—Å—Ç–∏                       –ó–∞–≥–æ–ª–æ–≤–æ–∫  –≠–º–±–µ–¥–¥–∏–Ω–≥_1  –≠–º–±–µ–¥–¥–∏–Ω–≥_2  \\\n",
            "0           1  –í–Ω–µ–±—Ä–∞—á–Ω–∞—è –¥–æ—á—å –ì–ª—ã–∑–∏–Ω–∞ —Ä–∞—Å–ø–ª–∞       0.3266       0.1348   \n",
            "1           2  –í–æ—Ä –ø—Ä–æ–≥–ª–æ—Ç–∏–ª –æ–≥—Ä–æ–º–Ω–æ–µ –∑–æ–ª–æ—Ç–æ–µ       0.3266       0.1348   \n",
            "2           3  –ü—É—Ç–∏–Ω –æ–±—ä—è—Å–Ω–∏–ª –≤–∫–ª—é—á–µ–Ω–∏–µ –ö—Ä—ã–º–∞       0.3266       0.1348   \n",
            "3           4  –≠–∫—Å–ø–µ—Ä—Ç—ã —Å–æ—Å—Ç–∞–≤–∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–∞–º—ã       0.3266       0.1348   \n",
            "4           5  –†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª Snap       0.3266       0.1348   \n",
            "\n",
            "   –≠–º–±–µ–¥–¥–∏–Ω–≥_3  –≠–º–±–µ–¥–¥–∏–Ω–≥_4  –≠–º–±–µ–¥–¥–∏–Ω–≥_5  –≠–º–±–µ–¥–¥–∏–Ω–≥_6  –≠–º–±–µ–¥–¥–∏–Ω–≥_7  \\\n",
            "0       0.0252       -0.427      -0.3547      -0.2825      -0.2793   \n",
            "1       0.0252       -0.427      -0.3547      -0.2825      -0.2793   \n",
            "2       0.0252       -0.427      -0.3547      -0.2825      -0.2793   \n",
            "3       0.0252       -0.427      -0.3547      -0.2825      -0.2793   \n",
            "4       0.0252       -0.427      -0.3547      -0.2825      -0.2793   \n",
            "\n",
            "   –≠–º–±–µ–¥–¥–∏–Ω–≥_8  –≠–º–±–µ–¥–¥–∏–Ω–≥_9  –≠–º–±–µ–¥–¥–∏–Ω–≥_10  \n",
            "0       0.0409      -0.4386        -0.292  \n",
            "1       0.0409      -0.4386        -0.292  \n",
            "2       0.0409      -0.4386        -0.292  \n",
            "3       0.0409      -0.4386        -0.292  \n",
            "4       0.0409      -0.4386        -0.292  \n",
            "\n",
            "\n",
            "5. –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ò\n",
            "----------------------------------------\n",
            "                                    –ü–∞—Ä–∞–º–µ—Ç—Ä  –ó–Ω–∞—á–µ–Ω–∏–µ –ï–¥–∏–Ω–∏—Ü–∞_–∏–∑–º–µ—Ä–µ–Ω–∏—è\n",
            "0                        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π        30               —à—Ç.\n",
            "1                      –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤       180              —Å–ª–æ–≤\n",
            "2                            –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤         6              —Å–ª–æ–≤\n",
            "3                   –°–ª–æ–≤ –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ         6              —Å–ª–æ–≤\n",
            "4                        –°–ª–æ–≤ –≤ Bag of Words         6              —Å–ª–æ–≤\n",
            "5                    –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤        10         –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
            "6            –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª–æ–≤)        50          —Å–∏–º–≤–æ–ª–æ–≤\n",
            "7  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤)         6              —Å–ª–æ–≤\n",
            "\n",
            "\n",
            "6. –¢–ê–ë–õ–ò–¶–ê –ü–†–ò–ú–ï–†–û–í –û–ë–†–ê–ë–û–¢–ö–ò\n",
            "----------------------------------------\n",
            "  –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ_—Å–ª–æ–≤–æ –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –û–∂–∏–¥–∞–µ–º—ã–π_—Ä–µ–∑—É–ª—å—Ç–∞—Ç –°—Ç–∞—Ç—É—Å\n",
            "0            –≥–æ–≤–æ—Ä–∏–ª          –≥–æ–≤–æ—Ä–∏—Ç—å            –≥–æ–≤–æ—Ä–∏—Ç—å      ‚úì\n",
            "1             —Å–∫–∞–∑–∞–ª           —Å–∫–∞–∑–∞—Ç—å             —Å–∫–∞–∑–∞—Ç—å      ‚úì\n",
            "2             —Ä–æ—Å—Å–∏–∏            —Ä–æ—Å—Å–∏—è              —Ä–æ—Å—Å–∏—è      ‚úì\n",
            "3             –º–æ—Å–∫–≤–µ            –º–æ—Å–∫–≤–∞              –º–æ—Å–∫–≤–∞      ‚úì\n",
            "4      –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞     –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ       –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ      ‚úì\n",
            "5      —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π     —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π       —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π      ‚úì\n",
            "6       –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è      –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ        –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ      ‚úì\n",
            "7          –∏–∑–º–µ–Ω–µ–Ω–∏—è          –∏–∑–º–µ–Ω–µ–Ω–∏           –∏–∑–º–µ–Ω–µ–Ω–∏–µ      ~\n",
            "8          –ø–æ–¥–ø–∏—Å–∞–ª–∏          –ø–æ–¥–ø–∏—Å–∞–ª           –ø–æ–¥–ø–∏—Å–∞—Ç—å      ~\n",
            "9         —Å–æ—Å—Ç–æ—è–ª–∞—Å—å          —Å–æ—Å—Ç–æ—è–ª–∞          —Å–æ—Å—Ç–æ—è—Ç—å—Å—è      ~\n",
            "\n",
            "\n",
            "7. –°–û–•–†–ê–ù–ï–ù–ò–ï –¢–ê–ë–õ–ò–¶ –í –§–ê–ô–õ–´\n",
            "----------------------------------------\n",
            "‚úì –Ω–æ–≤–æ—Å—Ç–∏.csv: 30 —Å—Ç—Ä–æ–∫, 5 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "‚úì —á–∞—Å—Ç–æ—Ç–Ω—ã–π_—Å–ª–æ–≤–∞—Ä—å.csv: 6 —Å—Ç—Ä–æ–∫, 3 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "‚úì bag_of_words.csv: 30 —Å—Ç—Ä–æ–∫, 8 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "‚úì —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.csv: 30 —Å—Ç—Ä–æ–∫, 12 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "‚úì —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.csv: 8 —Å—Ç—Ä–æ–∫, 3 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "‚úì –ø—Ä–∏–º–µ—Ä—ã_–æ–±—Ä–∞–±–æ—Ç–∫–∏.csv: 10 —Å—Ç—Ä–æ–∫, 4 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "\n",
            "======================================================================\n",
            "–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –í –¢–ê–ë–õ–ò–ß–ù–û–ú –§–û–†–ú–ê–¢–ï\n",
            "======================================================================\n",
            "\n",
            "–°–û–ó–î–ê–ù–ù–´–ï –¢–ê–ë–õ–ò–¶–´:\n",
            "                 –§–∞–π–ª                                              –û–ø–∏—Å–∞–Ω–∏–µ  –°—Ç—Ä–æ–∫  –°—Ç–æ–ª–±—Ü–æ–≤  –†–∞–∑–º–µ—Ä\n",
            "          –Ω–æ–≤–æ—Å—Ç–∏.csv –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏     30         5 19.6 KB\n",
            "—á–∞—Å—Ç–æ—Ç–Ω—ã–π_—Å–ª–æ–≤–∞—Ä—å.csv                    –¢–∞–±–ª–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ (—Ç–æ–ø-100)      6         3  0.7 KB\n",
            "     bag_of_words.csv                     –ú–∞—Ç—Ä–∏—Ü–∞ Bag of Words –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π     30         8  5.5 KB\n",
            "       —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.csv                       –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤     30        12  6.4 KB\n",
            "       —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.csv                          –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º      8         3  1.7 KB\n",
            "–ø—Ä–∏–º–µ—Ä—ã_–æ–±—Ä–∞–±–æ—Ç–∫–∏.csv                             –ü—Ä–∏–º–µ—Ä—ã –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤     10         4  3.2 KB\n",
            "\n",
            "\n",
            "–¢–û–ü-10 –°–õ–û–í –ü–û –ß–ê–°–¢–û–¢–ï:\n",
            " –†–∞–Ω–≥  –°–ª–æ–≤–æ  –ß–∞—Å—Ç–æ—Ç–∞  –ß–∞—Å—Ç–æ—Ç–∞_%\n",
            "    1  —Ç–µ–∫—Å—Ç       30       16.7\n",
            "    2 –Ω–æ–≤–æ—Å—Ç       30       16.7\n",
            "    3 –≤–∞–∂–Ω—ã—Ö       30       16.7\n",
            "    4 —Å–æ–±—ã—Ç–∏       30       16.7\n",
            "    5 —Ä–æ—Å—Å–∏—è       30       16.7\n",
            "    6    –º–∏—Ä       30       16.7\n",
            "\n",
            "======================================================================\n",
            "–í–°–ï –¢–ê–ë–õ–ò–¶–´ –£–°–ü–ï–®–ù–û –°–û–ó–î–ê–ù–´ –ò –°–û–•–†–ê–ù–ï–ù–´ –í CSV –§–ê–ô–õ–´!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã NLTK –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "    nltk.download('punkt')\n",
        "\n",
        "BASE = \"https://lenta.ru\"\n",
        "\n",
        "# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
        "STOP_WORDS = {\n",
        "    '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '—á—Ç–æ', '–Ω–µ', '–æ–Ω', '—ç—Ç–æ', '–∫–∞–∫', '–¥–ª—è', '—Ç–æ',\n",
        "    '–Ω–æ', '–∞', '–∏–∑', '–æ—Ç', '–∑–∞', '–∫', '–¥–æ', '–∂–µ', '–±—ã', '–≤—ã', '—É', '–æ', '—Å–æ',\n",
        "    '–ª–∏', '–∏–ª–∏', '–Ω–∏', '–¥–∞', '—Ç–∞–∫–æ–π', '—Ç–∞–º', '—Ç–∞–∫', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '—Ç–æ–∂–µ',\n",
        "    '—Ç–æ–ª—å–∫–æ', '—è', '–º—ã', '–æ–Ω–∏', '–æ–Ω–∞', '–æ–Ω–æ', '–º–æ–π', '—Ç–≤–æ–π', '—Å–≤–æ–π', '–Ω–∞—à',\n",
        "    '–≤–∞—à', '–≤–µ—Å—å', '—Å–∞–º', '—Å–µ–±—è', '–∫—Ç–æ', '–≥–¥–µ', '–∫—É–¥–∞', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É',\n",
        "    '–∑–∞—á–µ–º', '—Å–∫–æ–ª—å–∫–æ', '–æ—á–µ–Ω—å', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–¥–æ–ª–∂–µ–Ω', '–º–æ–≥—É—Ç', '–±—É–¥–µ—Ç',\n",
        "    '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–Ω–µ—Ç', '–≤–æ—Ç', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '—Ç–∞–º',\n",
        "    '—Å–µ–≥–æ–¥–Ω—è', '–≤—Ä–µ–º—è', '–≥–æ–¥', '–ø–æ—Å–ª–µ', '—á–µ—Ä–µ–∑', '–±–æ–ª–µ–µ', '–º–µ–Ω–µ–µ', '—Ç–æ–≥–¥–∞',\n",
        "    '—Å–µ–π—á–∞—Å', '–ø–æ—Ç–æ–º', '–∫–æ—Ç–æ—Ä—ã–µ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã—Ö', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–º—É',\n",
        "    '—ç—Ç–æ–º', '—ç—Ç–æ–π', '—ç—Ç–æ—Ç', '—ç—Ç–∏', '—ç—Ç—É', '—Ç–∞–∫–∏–µ', '—Ç–∞–∫–∏—Ö', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–æ–º',\n",
        "    '—Ç–∞–∫–æ–º—É', '—Ç–∞–∫–æ—é', '—Ç–∞–∫–æ–µ', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–æ–º', '—Ç–µ–ø–µ—Ä—å', '—Ç—É–¥–∞', '—Å—é–¥–∞',\n",
        "    '–æ–ø—è—Ç—å', '—É–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–≤–µ–¥—å', '–≤–ø—Ä–æ—á–µ–º', '–∫–æ–Ω–µ—á–Ω–æ', '–º–æ–∂–µ—Ç',\n",
        "    '–Ω–∞–∫–æ–Ω–µ—Ü', '–æ–¥–Ω–∞–∫–æ', '–ø—Ä–∏–º–µ—Ä–Ω–æ', '–ø—Ä–∏–º–µ—Ä–æ–º', '–ø—Ä–æ—Å—Ç–æ', '—Å—Ä–∞–∑—É', '—Å–æ–≤—Å–µ–º',\n",
        "    '—Ö–æ—Ç—è', '—á—É—Ç—å', '–≤–æ–æ–±—â–µ', '–≤—Å—ë', '–≤—Å–µ', '–≤—Å–µ–≥–æ', '–≤—Å–µ–º', '–≤—Å–µ–º–∏', '–≤—Å–µ–º—É',\n",
        "    '–≤—Å–µ—Ö', '–≤—Å–µ—é', '–≤—Å–µ—è', '–≤—Å—é', '–≤—Å—è', '–≤—Å—ë', '–µ—â—ë', '–µ—â–µ', '–µ—Å–ª–∏', '–∏–ª–∏',\n",
        "    '–∞', '–Ω–æ', '–±—ã', '–∂–µ', '–ª–∏', '–Ω–∏', '–ø—É—Å—Ç—å', '—á—Ç–æ–±', '—á—Ç–æ–±—ã', '–∫–∞–∫', '—Ç–∞–∫',\n",
        "    '—Ç–∞–∫–∂–µ', '—Ç–æ–º', '—Ç–æ–º', '—Ç–∞–º', '—Ç—É—Ç', '—á–µ–º', '—á—Ç–æ', '—ç—Ç–æ', '—ç—Ç–æ–º', '—ç—Ç–æ—Ç'\n",
        "}\n",
        "\n",
        "def simple_lemmatize(word):\n",
        "    \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\"\"\"\n",
        "    # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –∏ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞\n",
        "    exceptions = {\n",
        "        '—ç—Ç': '—ç—Ç–æ—Ç', '—Å–≤–æ': '—Å–≤–æ–π', '–∫–æ—Ç–æ—Ä': '–∫–æ—Ç–æ—Ä—ã–π', '—Ç–∞–∫–∂': '—Ç–∞–∫–∂–µ',\n",
        "        '–∑–∞—è–≤': '–∑–∞—è–≤–ª—è—Ç—å', '–≥–æ–≤–æ—Ä': '–≥–æ–≤–æ—Ä–∏—Ç—å', '—Å–∫–∞–∑': '—Å–∫–∞–∑–∞—Ç—å',\n",
        "        '—Å—á–∏—Ç–∞': '—Å—á–∏—Ç–∞—Ç—å', '–ø–æ–∫–∞–∑': '–ø–æ–∫–∞–∑–∞—Ç—å', '–æ–±—ä—è—Å–Ω': '–æ–±—ä—è—Å–Ω–∏—Ç—å',\n",
        "        '–Ω–∞–∑–≤': '–Ω–∞–∑–≤–∞—Ç—å', '—Å–º–æ—Ç—Ä': '—Å–º–æ—Ç—Ä–µ—Ç—å', '–¥—É–º': '–¥—É–º–∞—Ç—å',\n",
        "        '–∑–Ω–∞': '–∑–Ω–∞—Ç—å', '—Ö–æ—Ç': '—Ö–æ—Ç–µ—Ç—å', '–º–æ–∂': '–º–æ—á—å', '—Å—Ç–∞–≤': '—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è',\n",
        "        '—Ä–æ—Å—Å–∏': '—Ä–æ—Å—Å–∏—è', '–º–æ—Å–∫–≤': '–º–æ—Å–∫–≤–∞', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤': '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ',\n",
        "        '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤': '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç': '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç',\n",
        "        '–ø—Ä–∞–≤–∏—Ç–µ–ª': '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '—Å—Ç—Ä–∞': '—Å—Ç—Ä–∞–Ω–∞', '–≥–æ—Ä–æ–¥': '–≥–æ—Ä–æ–¥',\n",
        "        '—á–µ–ª–æ–≤–µ–∫': '—á–µ–ª–æ–≤–µ–∫', '–∫–æ–º–ø–∞–Ω–∏': '–∫–æ–º–ø–∞–Ω–∏—è', '—Ä–∞–±–æ—Ç': '—Ä–∞–±–æ—Ç–∞',\n",
        "        '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫': '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω': '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π',\n",
        "        '—Ç–µ—Ö–Ω–æ–ª–æ–≥': '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω': '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ',\n",
        "        '—Å–æ—Ü–∏–∞–ª—å–Ω': '—Å–æ—Ü–∏–∞–ª—å–Ω—ã–π', '–ø–æ–ª–∏—Ç–∏–∫': '–ø–æ–ª–∏—Ç–∏–∫–∞', '–±–∞–Ω–∫': '–±–∞–Ω–∫',\n",
        "        '–∫—Ä–µ–¥–∏—Ç': '–∫—Ä–µ–¥–∏—Ç', '–ø—Ä–æ—Ü–µ–Ω—Ç': '–ø—Ä–æ—Ü–µ–Ω—Ç', '—Å—Ç–∞–≤–∫': '—Å—Ç–∞–≤–∫–∞',\n",
        "        '–Ω–æ–≤–æ—Å—Ç': '–Ω–æ–≤–æ—Å—Ç—å', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü': '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '—Ä–∞–∑–≤–∏—Ç': '—Ä–∞–∑–≤–∏—Ç–∏–µ',\n",
        "        '–ø—Ä–æ–µ–∫—Ç': '–ø—Ä–æ–µ–∫—Ç', '—Å–∏—Å—Ç–µ–º': '—Å–∏—Å—Ç–µ–º–∞', '–ø—Ä–æ–≥—Ä–∞–º–º': '–ø—Ä–æ–≥—Ä–∞–º–º–∞',\n",
        "        '–≤—Å—Ç—Ä–µ—á': '–≤—Å—Ç—Ä–µ—á–∞', '–¥–æ–≥–æ–≤–æ—Ä': '–¥–æ–≥–æ–≤–æ—Ä', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤': '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ',\n",
        "        '—Å–æ–±—ã—Ç': '—Å–æ–±—ã—Ç–∏–µ', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç': '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ'\n",
        "    }\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è\n",
        "    for wrong, correct in exceptions.items():\n",
        "        if word.startswith(wrong):\n",
        "            return correct\n",
        "\n",
        "    # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "    endings = [\n",
        "        ('–∞—è', '—ã–π'), ('–æ–µ', '–æ–π'), ('–∏–µ', '–∏–π'), ('—ã–µ', '—ã–π'),\n",
        "        ('–∞', ''), ('—è', ''), ('–æ', ''), ('–µ', ''), ('–∏', ''), ('—ã', ''),\n",
        "        ('—É', ''), ('—é', ''), ('–æ–π', ''), ('–µ–π', ''), ('–æ–º', ''), ('–µ–º', ''),\n",
        "        ('–∞–º', ''), ('—è–º', ''), ('–∞—Ö', ''), ('—è—Ö', ''), ('–∏–π', '–π'),\n",
        "        ('—ã–π', '–π'), ('–æ–π', '–π'), ('—Ç—å', ''), ('—Ç–∏', ''), ('—á—å', ''),\n",
        "        ('—Å—è', ''), ('—Å—å', ''), ('–ª–∞', '—Ç—å'), ('–ª–æ', '—Ç—å'), ('–ª–∏', '—Ç—å'),\n",
        "        ('–µ–º', '—Ç—å'), ('–µ—Ç–µ', '—Ç—å'), ('–µ—Ç', '—Ç—å'), ('—é—Ç', '—Ç—å'), ('–∏–º', '—Ç—å'),\n",
        "        ('–∏—Ç–µ', '—Ç—å'), ('–∏—Ç', '—Ç—å'), ('–∞—Ç', '—Ç—å'), ('—è—Ç', '—Ç—å')\n",
        "    ]\n",
        "\n",
        "    for ending, replacement in endings:\n",
        "        if word.endswith(ending):\n",
        "            if replacement:\n",
        "                return word[:-len(ending)] + replacement\n",
        "            else:\n",
        "                return word[:-len(ending)]\n",
        "\n",
        "    return word\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "    text = text.lower()\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Ü–∏—Ñ—Ä—ã\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é NLTK\n",
        "    try:\n",
        "        tokens = word_tokenize(text, language='russian')\n",
        "    except:\n",
        "        tokens = text.split()\n",
        "\n",
        "    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
        "    processed_tokens = []\n",
        "    for token in tokens:\n",
        "        if len(token) >= 3 and token not in STOP_WORDS:\n",
        "            lemma = simple_lemmatize(token)\n",
        "            if len(lemma) >= 3 and lemma not in STOP_WORDS:\n",
        "                processed_tokens.append(lemma)\n",
        "\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "def parse_full_text(url: str) -> str:\n",
        "    \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Å—Å—ã–ª–∫–µ.\"\"\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "\n",
        "        # –ò—â–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å —Ç–µ–∫—Å—Ç–æ–º\n",
        "        blocks = soup.find_all(\"p\", class_=lambda x: x and \"body\" in x)\n",
        "        if not blocks:\n",
        "            blocks = soup.find_all(\"div\", class_=lambda x: x and \"body\" in x)\n",
        "        if not blocks:\n",
        "            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏\n",
        "            article = soup.find(\"article\")\n",
        "            if article:\n",
        "                blocks = article.find_all(\"p\")\n",
        "\n",
        "        text = \" \".join(b.get_text(strip=True) for b in blocks if b.get_text(strip=True))\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def parse_news(news_count=50, date_str: str | None = None) -> pd.DataFrame:\n",
        "    \"\"\"–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å Lenta.ru\"\"\"\n",
        "    if date_str:\n",
        "        y, m, d = map(int, date_str.split(\"-\"))\n",
        "        url = f\"{BASE}/news/{y:04d}/{m:02d}/{d:02d}/\"\n",
        "    else:\n",
        "        url = BASE + \"/\"\n",
        "\n",
        "    r = requests.get(url, timeout=10)\n",
        "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "    news = []\n",
        "\n",
        "    if date_str:\n",
        "        # –í—Å–µ —Å—Å—ã–ª–∫–∏, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ —ç—Ç–æ–π –¥–∞—Ç–µ\n",
        "        prefix = f\"/news/{y:04d}/{m:02d}/{d:02d}/\"\n",
        "        items = [a for a in soup.find_all(\"a\", href=True)\n",
        "                 if a[\"href\"].startswith(prefix)]\n",
        "    else:\n",
        "        # –ö–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ –≥–ª–∞–≤–Ω–æ–π\n",
        "        items = []\n",
        "        # –ò—â–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –∫–∞—Ä—Ç–æ—á–µ–∫\n",
        "        for class_name in [\"card-mini\", \"card-big\", \"card-full\", \"card-\", \"topnews\"]:\n",
        "            items.extend(soup.find_all(\"a\", class_=lambda x: x and class_name in x))\n",
        "\n",
        "        # –ï—Å–ª–∏ –º–∞–ª–æ –Ω–∞–π–¥–µ–Ω–æ, –±–µ—Ä–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å /news/\n",
        "        if len(items) < news_count:\n",
        "            all_links = soup.find_all(\"a\", href=True)\n",
        "            items = [a for a in all_links if \"/news/\" in a[\"href\"]]\n",
        "\n",
        "    for a in items:\n",
        "        if len(news) >= news_count:\n",
        "            break\n",
        "\n",
        "        # –∑–∞–≥–æ–ª–æ–≤–æ–∫\n",
        "        title_elem = a.find([\"h3\", \"span\", \"div\"]) or a\n",
        "        title = title_elem.get_text(strip=True)\n",
        "        if not title or len(title) < 10:\n",
        "            continue\n",
        "\n",
        "        # —Å—Å—ã–ª–∫–∞\n",
        "        link = a.get(\"href\")\n",
        "        if not link:\n",
        "            continue\n",
        "\n",
        "        if link and not link.startswith(\"http\"):\n",
        "            link = BASE + link\n",
        "\n",
        "        full_text = parse_full_text(link) if link else \"\"\n",
        "\n",
        "        if not full_text or len(full_text) < 50:\n",
        "            continue\n",
        "\n",
        "        news.append({\n",
        "            \"date\": date_str or dt.date.today().isoformat(),\n",
        "            \"title\": title,\n",
        "            \"link\": link,\n",
        "            \"full_text\": full_text\n",
        "        })\n",
        "\n",
        "        print(f\"–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news)}/{news_count}\")\n",
        "\n",
        "    return pd.DataFrame(news)\n",
        "\n",
        "def create_frequency_dict(texts):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è\"\"\"\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        all_words.extend(text.split())\n",
        "\n",
        "    word_counts = Counter(all_words)\n",
        "    return dict(word_counts.most_common(200))\n",
        "\n",
        "def create_bag_of_words(texts, vocabulary):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ Bag of Words –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è\"\"\"\n",
        "    bow_matrix = []\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        word_freq = Counter(words)\n",
        "        vector = [word_freq.get(word, 0) for word in vocabulary]\n",
        "        bow_matrix.append(vector)\n",
        "\n",
        "    return bow_matrix\n",
        "\n",
        "def create_embeddings(texts, embedding_dim=50):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)\"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        all_words.extend(text.split())\n",
        "\n",
        "    unique_words = list(set(all_words))\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ª–æ–≤ (—Å–ª—É—á–∞–π–Ω—ã–µ, –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)\n",
        "    word_embeddings = {}\n",
        "    for word in unique_words:\n",
        "        word_embeddings[word] = np.random.randn(embedding_dim)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ (—Å—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤)\n",
        "    text_embeddings = []\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        if words:\n",
        "            vectors = [word_embeddings[w] for w in words if w in word_embeddings]\n",
        "            if vectors:\n",
        "                text_embedding = np.mean(vectors, axis=0)\n",
        "            else:\n",
        "                text_embedding = np.zeros(embedding_dim)\n",
        "        else:\n",
        "            text_embedding = np.zeros(embedding_dim)\n",
        "        text_embeddings.append(text_embedding)\n",
        "\n",
        "    return np.array(text_embeddings), word_embeddings\n",
        "\n",
        "def main():\n",
        "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞–Ω–∏—è\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê: –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–í–û–°–¢–ï–ô\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # –®–∞–≥ 1: –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "    print(\"\\n1. –°–ë–û–† 50 –ù–û–í–û–°–¢–ï–ô –° LENTA.RU\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    date_input = input(\"–í–≤–µ–¥–∏—Ç–µ –¥–∞—Ç—É YYYY-MM-DD (–∏–ª–∏ Enter –¥–ª—è –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã): \").strip()\n",
        "\n",
        "    if date_input:\n",
        "        df = parse_news(news_count=50, date_str=date_input)\n",
        "    else:\n",
        "        df = parse_news(news_count=50)\n",
        "\n",
        "    print(f\"\\n‚úì –°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(df)}\")\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    df.to_csv(\"original_news.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"‚úì –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'original_news.csv'\")\n",
        "\n",
        "    # –®–∞–≥ 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
        "    print(\"\\n2. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–û–í\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    df['processed_text'] = df['full_text'].apply(preprocess_text)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã\n",
        "    df = df[df['processed_text'].str.len() > 20].copy()\n",
        "    print(f\"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(df)}\")\n",
        "\n",
        "    # –®–∞–≥ 3: –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "    print(\"\\n3. –°–û–ó–î–ê–ù–ò–ï –ß–ê–°–¢–û–¢–ù–û–ì–û –°–õ–û–í–ê–†–Ø\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    frequency_dict = create_frequency_dict(df['processed_text'])\n",
        "    vocabulary = list(frequency_dict.keys())\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è\n",
        "    freq_df = pd.DataFrame(\n",
        "        list(frequency_dict.items()),\n",
        "        columns=['word', 'frequency']\n",
        "    )\n",
        "    freq_df['rank'] = range(1, len(freq_df) + 1)\n",
        "\n",
        "    print(f\"‚úì –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–æ–∑–¥–∞–Ω: {len(frequency_dict)} —Å–ª–æ–≤\")\n",
        "    print(\"\\n–¢–æ–ø-10 —Å–ª–æ–≤:\")\n",
        "    print(freq_df.head(10)[['word', 'frequency']].to_string(index=False))\n",
        "\n",
        "    # –®–∞–≥ 4: Bag of Words\n",
        "    print(\"\\n4. BAG OF WORDS –ö–û–î–ò–†–û–í–ê–ù–ò–ï\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ø-100 —Å–ª–æ–≤ –¥–ª—è BoW\n",
        "    top_words = vocabulary[:100]\n",
        "    bow_matrix = create_bag_of_words(df['processed_text'], top_words)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è BoW\n",
        "    bow_df = pd.DataFrame(bow_matrix, columns=top_words)\n",
        "    bow_df.insert(0, 'news_id', range(1, len(bow_df) + 1))\n",
        "    bow_df.insert(1, 'title', df['title'].values)\n",
        "\n",
        "    print(f\"‚úì Bag of Words —Å–æ–∑–¥–∞–Ω: {len(bow_matrix)} √ó {len(top_words)}\")\n",
        "    print(f\"  (–ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {', '.join(top_words[:5])})\")\n",
        "\n",
        "    # –®–∞–≥ 5: –≠–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "    print(\"\\n5. –°–û–ó–î–ê–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    embeddings, word_embeddings = create_embeddings(df['processed_text'], embedding_dim=50)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤\n",
        "    emb_df = pd.DataFrame(embeddings)\n",
        "    emb_df.columns = [f'embedding_{i}' for i in range(embeddings.shape[1])]\n",
        "    emb_df.insert(0, 'news_id', range(1, len(emb_df) + 1))\n",
        "    emb_df.insert(1, 'title', df['title'].values)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–ª–æ–≤\n",
        "    word_emb_df = pd.DataFrame(word_embeddings).T\n",
        "    word_emb_df.index.name = 'word'\n",
        "    word_emb_df.columns = [f'dim_{i}' for i in range(word_emb_df.shape[1])]\n",
        "\n",
        "    print(f\"‚úì –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã:\")\n",
        "    print(f\"  - –¢–µ–∫—Å—Ç–æ–≤: {embeddings.shape[0]} √ó {embeddings.shape[1]}\")\n",
        "    print(f\"  - –°–ª–æ–≤: {word_emb_df.shape[0]} √ó {word_emb_df.shape[1]}\")\n",
        "\n",
        "    # –®–∞–≥ 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    print(\"\\n6. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏\n",
        "    df_output = df[['date', 'title', 'link', 'full_text', 'processed_text']].copy()\n",
        "    df_output.to_csv('news_processed.csv', index=False, encoding='utf-8-sig')\n",
        "    print(\"‚úì 'news_processed.csv' - –Ω–æ–≤–æ—Å—Ç–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏\")\n",
        "\n",
        "    # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "    freq_df.to_csv('frequency_dictionary.csv', index=False, encoding='utf-8-sig')\n",
        "    print(\"‚úì 'frequency_dictionary.csv' - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\")\n",
        "\n",
        "    # Bag of Words\n",
        "    bow_df.to_csv('bag_of_words.csv', index=False, encoding='utf-8-sig')\n",
        "    print(\"‚úì 'bag_of_words.csv' - –º–∞—Ç—Ä–∏—Ü–∞ Bag of Words\")\n",
        "\n",
        "    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
        "    emb_df.to_csv('text_embeddings.csv', index=False, encoding='utf-8-sig')\n",
        "    print(\"‚úì 'text_embeddings.csv' - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤\")\n",
        "\n",
        "    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤\n",
        "    word_emb_df.to_csv('word_embeddings.csv', encoding='utf-8-sig')\n",
        "    print(\"‚úì 'word_embeddings.csv' - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤\")\n",
        "\n",
        "    # –®–∞–≥ 7: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –ø—Ä–∏–º–µ—Ä—ã\n",
        "    print(\"\\n7. –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –ü–†–ò–ú–ï–†–´\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    stats_data = {\n",
        "        '–ü–∞—Ä–∞–º–µ—Ç—Ä': [\n",
        "            '–í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π',\n",
        "            '–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ',\n",
        "            '–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤',\n",
        "            '–°–ª–æ–≤ –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ',\n",
        "            '–°–ª–æ–≤ –≤ Bag of Words',\n",
        "            '–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤',\n",
        "            '–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤)',\n",
        "            '–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤)'\n",
        "        ],\n",
        "        '–ó–Ω–∞—á–µ–Ω–∏–µ': [\n",
        "            len(df),\n",
        "            len(df_output),\n",
        "            len(set(' '.join(df['processed_text']).split())),\n",
        "            len(frequency_dict),\n",
        "            len(top_words),\n",
        "            embeddings.shape[1],\n",
        "            int(df['full_text'].apply(lambda x: len(x.split())).mean()),\n",
        "            int(df['processed_text'].apply(lambda x: len(x.split())).mean())\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    stats_df = pd.DataFrame(stats_data)\n",
        "    stats_df.to_csv('statistics.csv', index=False, encoding='utf-8-sig')\n",
        "    print(\"‚úì 'statistics.csv' - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º\")\n",
        "    print(\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "    print(stats_df.to_string(index=False))\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "    print(\"\\n–ü—Ä–∏–º–µ—Ä—ã –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏:\")\n",
        "    examples = [\n",
        "        (\"–≥–æ–≤–æ—Ä–∏–ª\", \"–≥–æ–≤–æ—Ä–∏—Ç—å\"),\n",
        "        (\"—Å–∫–∞–∑–∞–ª\", \"—Å–∫–∞–∑–∞—Ç—å\"),\n",
        "        (\"—Ä–æ—Å—Å–∏–∏\", \"—Ä–æ—Å—Å–∏—è\"),\n",
        "        (\"–º–æ—Å–∫–≤–µ\", \"–º–æ—Å–∫–≤–∞\"),\n",
        "        (\"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞\", \"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ\"),\n",
        "        (\"—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π\", \"—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π\")\n",
        "    ]\n",
        "\n",
        "    for original, expected in examples:\n",
        "        result = simple_lemmatize(original)\n",
        "        status = \"‚úì\" if result == expected else \"~\"\n",
        "        print(f\"  {status} {original:15} ‚Üí {result:15} (–æ–∂–∏–¥–∞–ª–æ—Å—å: {expected})\")\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä—ã –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    print(\"\\n–ü—Ä–∏–º–µ—Ä—ã –∏–∑ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:\")\n",
        "    print(\"–ü–µ—Ä–≤—ã–µ 3 –∑–∞–≥–æ–ª–æ–≤–∫–∞:\")\n",
        "    for i, title in enumerate(df['title'].head(3), 1):\n",
        "        print(f\"  {i}. {title}\")\n",
        "\n",
        "    print(\"\\n–ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤):\")\n",
        "    if len(df) > 0:\n",
        "        sample_text = df.iloc[0]['full_text'][:100]\n",
        "        sample_processed = df.iloc[0]['processed_text'][:100]\n",
        "        print(f\"  –û—Ä–∏–≥–∏–Ω–∞–ª: {sample_text}...\")\n",
        "        print(f\"  –û–±—Ä–∞–±–æ—Ç–∞–Ω: {sample_processed}...\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"–í–´–ü–û–õ–ù–ï–ù–ò–ï –ó–ê–î–ê–ù–ò–Ø –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\n–°–æ–∑–¥–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã:\")\n",
        "    print(\"1. original_news.csv - –∏—Å—Ö–æ–¥–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏\")\n",
        "    print(\"2. news_processed.csv - –Ω–æ–≤–æ—Å—Ç–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏\")\n",
        "    print(\"3. frequency_dictionary.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\")\n",
        "    print(\"4. bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ Bag of Words\")\n",
        "    print(\"5. text_embeddings.csv - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤\")\n",
        "    print(\"6. word_embeddings.csv - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤\")\n",
        "    print(\"7. statistics.csv - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º\")\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VBxpVQzI2sW",
        "outputId": "ece691e3-ae5a-409f-ff05-5ab082d2900d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê: –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–í–û–°–¢–ï–ô\n",
            "======================================================================\n",
            "\n",
            "1. –°–ë–û–† 50 –ù–û–í–û–°–¢–ï–ô –° LENTA.RU\n",
            "----------------------------------------\n",
            "–í–≤–µ–¥–∏—Ç–µ –¥–∞—Ç—É YYYY-MM-DD (–∏–ª–∏ Enter –¥–ª—è –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã): 2006-12-26\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 1/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 2/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 3/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 4/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 5/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 6/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 7/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 8/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 9/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 10/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 11/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 12/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 13/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 14/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 15/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 16/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 17/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 18/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 19/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 20/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 21/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 22/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 23/50\n",
            "–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 24/50\n",
            "\n",
            "‚úì –°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 24\n",
            "‚úì –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'original_news.csv'\n",
            "\n",
            "2. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–û–í\n",
            "----------------------------------------\n",
            "‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: 24\n",
            "\n",
            "3. –°–û–ó–î–ê–ù–ò–ï –ß–ê–°–¢–û–¢–ù–û–ì–û –°–õ–û–í–ê–†–Ø\n",
            "----------------------------------------\n",
            "‚úì –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–æ–∑–¥–∞–Ω: 200 —Å–ª–æ–≤\n",
            "\n",
            "–¢–æ–ø-10 —Å–ª–æ–≤:\n",
            "       word  frequency\n",
            "     —Ä–æ—Å—Å–∏—è         37\n",
            "     —Å—Ç—Ä–∞–Ω–∞         23\n",
            "  –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç         21\n",
            "     –¥–µ–∫–∞–±—Ä         19\n",
            "    –≤—Ç–æ—Ä–Ω–∏–∫         18\n",
            "       –ø–æ—Å—Ç         15\n",
            "      —Ç—ã—Å—è—á         13\n",
            "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫         13\n",
            "   –Ω–∞—Ä–æ–¥–Ω–æ–≥         13\n",
            "   —Å–æ–æ–±—â–∞—Ç—å         11\n",
            "\n",
            "4. BAG OF WORDS –ö–û–î–ò–†–û–í–ê–ù–ò–ï\n",
            "----------------------------------------\n",
            "‚úì Bag of Words —Å–æ–∑–¥–∞–Ω: 24 √ó 100\n",
            "  (–ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: —Ä–æ—Å—Å–∏—è, —Å—Ç—Ä–∞–Ω–∞, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç, –¥–µ–∫–∞–±—Ä, –≤—Ç–æ—Ä–Ω–∏–∫)\n",
            "\n",
            "5. –°–û–ó–î–ê–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í\n",
            "----------------------------------------\n",
            "‚úì –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã:\n",
            "  - –¢–µ–∫—Å—Ç–æ–≤: 24 √ó 50\n",
            "  - –°–ª–æ–≤: 1596 √ó 50\n",
            "\n",
            "6. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
            "----------------------------------------\n",
            "‚úì 'news_processed.csv' - –Ω–æ–≤–æ—Å—Ç–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏\n",
            "‚úì 'frequency_dictionary.csv' - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
            "‚úì 'bag_of_words.csv' - –º–∞—Ç—Ä–∏—Ü–∞ Bag of Words\n",
            "‚úì 'text_embeddings.csv' - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
            "‚úì 'word_embeddings.csv' - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤\n",
            "\n",
            "7. –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –ü–†–ò–ú–ï–†–´\n",
            "----------------------------------------\n",
            "‚úì 'statistics.csv' - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º\n",
            "\n",
            "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
            "                                 –ü–∞—Ä–∞–º–µ—Ç—Ä  –ó–Ω–∞—á–µ–Ω–∏–µ\n",
            "                           –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π        24\n",
            "                       –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ        24\n",
            "                          –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤      1596\n",
            "                 –°–ª–æ–≤ –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ       200\n",
            "                      –°–ª–æ–≤ –≤ Bag of Words       100\n",
            "                  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤        50\n",
            "              –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤)       162\n",
            "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤)       116\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä—ã –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏:\n",
            "  ‚úì –≥–æ–≤–æ—Ä–∏–ª         ‚Üí –≥–æ–≤–æ—Ä–∏—Ç—å        (–æ–∂–∏–¥–∞–ª–æ—Å—å: –≥–æ–≤–æ—Ä–∏—Ç—å)\n",
            "  ‚úì —Å–∫–∞–∑–∞–ª          ‚Üí —Å–∫–∞–∑–∞—Ç—å         (–æ–∂–∏–¥–∞–ª–æ—Å—å: —Å–∫–∞–∑–∞—Ç—å)\n",
            "  ‚úì —Ä–æ—Å—Å–∏–∏          ‚Üí —Ä–æ—Å—Å–∏—è          (–æ–∂–∏–¥–∞–ª–æ—Å—å: —Ä–æ—Å—Å–∏—è)\n",
            "  ‚úì –º–æ—Å–∫–≤–µ          ‚Üí –º–æ—Å–∫–≤–∞          (–æ–∂–∏–¥–∞–ª–æ—Å—å: –º–æ—Å–∫–≤–∞)\n",
            "  ‚úì –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞   ‚Üí –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ   (–æ–∂–∏–¥–∞–ª–æ—Å—å: –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ)\n",
            "  ‚úì —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π   ‚Üí —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π   (–æ–∂–∏–¥–∞–ª–æ—Å—å: —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π)\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä—ã –∏–∑ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:\n",
            "–ü–µ—Ä–≤—ã–µ 3 –∑–∞–≥–æ–ª–æ–≤–∫–∞:\n",
            "  1. –ö—Ä–µ–º–ª–µ–≤—Å–∫–∏–π –¥–≤–æ—Ä–µ—Ü –∑–∞—Å—Ç—Ä–∞—Ö—É—é—Ç –æ—Ç –≤–∑—Ä—ã–≤–∞ –≥–∞–∑–∞ –∏ —É–¥–∞—Ä–∞ –º–æ–ª–Ω–∏–∏\n",
            "  2. –û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞–º —Ä–µ—Ñ–µ—Ä–µ–Ω–¥—É–º–∞ –ø–æ \"–ì–∞–∑–ø—Ä–æ–º-—Å–∏—Ç–∏\" –æ—Ç–∫–∞–∑–∞–ª–∏ –≤ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏\n",
            "  3. –§–ê–° –∑–∞–ø—Ä–µ—Ç–∏–ª–∞ —Å–Ω–æ—Å–∏—Ç—å —Å–µ–º—å –ü–¢–£ –≤ —Ü–µ–Ω—Ç—Ä–µ –ú–æ—Å–∫–≤—ã\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤):\n",
            "  –û—Ä–∏–≥–∏–Ω–∞–ª: –ö—Ä–µ–º–ª–µ–≤—Å–∫–∏–π –¥–≤–æ—Ä–µ—Ü –≤ 2007 –≥–æ–¥—É, –∞ —Ç–∞–∫–∂–µ –µ–≥–æ –∏–Ω—Ç–µ—Ä—å–µ—Ä—ã –∏ –º–µ–±–µ–ª—å –±—É–¥—É—Ç –∑–∞—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω—ã –æ—Ç –ø–∞–¥–∞—é—â–∏—Ö —Å–∞–º–æ–ª–µ...\n",
            "  –û–±—Ä–∞–±–æ—Ç–∞–Ω: –∫—Ä–µ–º–ª–µ–≤—Å–∫–π –¥–≤–æ—Ä–µ—Ü –∏–Ω—Ç–µ—Ä—å–µ—Ä –º–µ–±–µ–ª—å –±—É–¥—É—Ç –∑–∞—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω –ø–∞–¥–∞—é—â–∏—Ö —Å–∞–º–æ–ª–µ—Ç–æ–≤ –æ–±–ª–æ–º–∫–æ–≤ –≤–∑—Ä—ã–≤ –≥–∞–∑ —É–¥–∞—Ä –º–æ–ª–Ω–∏...\n",
            "\n",
            "======================================================================\n",
            "–í–´–ü–û–õ–ù–ï–ù–ò–ï –ó–ê–î–ê–ù–ò–Ø –ó–ê–í–ï–†–®–ï–ù–û!\n",
            "======================================================================\n",
            "\n",
            "–°–æ–∑–¥–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã:\n",
            "1. original_news.csv - –∏—Å—Ö–æ–¥–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏\n",
            "2. news_processed.csv - –Ω–æ–≤–æ—Å—Ç–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏\n",
            "3. frequency_dictionary.csv - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
            "4. bag_of_words.csv - –º–∞—Ç—Ä–∏—Ü–∞ Bag of Words\n",
            "5. text_embeddings.csv - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n",
            "6. word_embeddings.csv - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤\n",
            "7. statistics.csv - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}